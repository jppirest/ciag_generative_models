{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Module 5 - CNN as an Autoencoder\n",
    "### The main idea of this notebook is to implement a CNN and use it as an autoencoder. We will use the toy dataset of MNIST.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"../images/cnn_autoencoder.png\" />\n",
    "</p>\n",
    "\n",
    "> Differently than the MLP autoencoder, we will use the autoencoder to denoise the input image.\n",
    "\n",
    "> We will basically add a watermark (or some sort of random noise) and try to remove it. The reconstruction loss will be used to recover the original image from the noised one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic imports and setting random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Chave:  ewav\n",
      "Senha: ········\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Variáveis de ambiente http_proxy e https_proxy configuradas!\n"
     ]
    }
   ],
   "source": [
    "%load_ext nbproxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = datasets.MNIST(root='/pgeoprj/ciag2023/datasets/mnist', train=True, download=True, transform=ToTensor(),)\n",
    "test_data = datasets.MNIST(root='/pgeoprj/ciag2023/datasets/mnist', train=False, download=True, transform=ToTensor(),)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize = (20, 4))\n",
    "cols, rows = 10, 1\n",
    "\n",
    "for i in range (0, cols*rows):\n",
    "    label_idx = torch.where(train_data.targets == i)[0]\n",
    "    random_idx = torch.randint(len(label_idx), size=(1,)).item()\n",
    "    sample_idx = label_idx[random_idx]\n",
    "    image, label = train_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image.squeeze(), cmap='gray')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding a watermark (noise) to images\n",
    "\n",
    "- To help adding a watermark to the images we will use a helper function named `add_symbol`, defined below.\n",
    "- We will add the watermark `CIAG` to each image, and our job will be to reconstruct the original image without the watermark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_symbol(images):\n",
    "    possible_coordinates = [(3,3), (3,25), (15,25), (15,3)]\n",
    "    \n",
    "    for image in images:\n",
    "        image = image.squeeze(0)\n",
    "        random_idx = np.random.randint(0,4)\n",
    "        x,y = possible_coordinates[random_idx]\n",
    "        \n",
    "        # Writting the letter C\n",
    "        image[y-2:y+2, x-1] = 1        \n",
    "        image[y-2,x-1:x+2]= 1\n",
    "        image[y+1,x-1:x+2]= 1\n",
    "        \n",
    "        # Writting the letter I\n",
    "        image[y-2:y+2, x+3] = 1\n",
    "        \n",
    "        # Writting the letter A\n",
    "        image[y-2:y+2, x+5] = 1\n",
    "        image[y-2:y+2, x+7] = 1\n",
    "        image[y-2, x+6] = 1\n",
    "        image[y, x+6] = 1\n",
    "        \n",
    "        # Writting the letter G\n",
    "        image[y-2:y+2, x+9] = 1        \n",
    "        image[y-2,x+9: x+12]= 1\n",
    "        image[y+1,x+9:x+12]= 1\n",
    "        \n",
    "        image[y,x+11]= 1\n",
    "        image = image.unsqueeze(0)\n",
    "        \n",
    "    return images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize = (20, 4))\n",
    "cols, rows = 10, 1\n",
    "\n",
    "for i in range (0, cols*rows):\n",
    "    label_idx = torch.where(train_data.targets == i)[0]\n",
    "    random_idx = torch.randint(len(label_idx), size=(1,)).item()\n",
    "    sample_idx = label_idx[random_idx]\n",
    "    image, label = train_data[sample_idx]\n",
    "    image = add_symbol(image)\n",
    "    figure.add_subplot(rows, cols, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image.squeeze(), cmap='gray')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architecture\n",
    "\n",
    "> To have a better code organization, our architecture will be made of three classes (blocks). One class that will define an `Encoder Convolutional Block`, another to define a `Decoder Convolutional Block` and the model itself, in which we unify those classes to build our architecture.\n",
    "\n",
    "- **Encoder Convolutional Block**: defined by `EncoderConvBlock` class, is composed basically of a double convolution followed by an activation function and then a pooling layer, to decrease the dimensionality of our data. The convolutions are going to be performed by the `nn.Conv2d` PyTorch module, which we instatiate as a `self` object, to build the network. As usual, we pass the `in_channels` as well as the `out_channels` to the convolution. We also use `kernel_size = 3` and `padding = 1`, to mantain the dimensions of our data (equivalent of `padding = \"same\"`). After each one of the two convolutions, a `nn.ReLU` is applied, to activate the inputs. Finally, as the last layer of our block, we pass a `nn.MaxPool2d` with `kernel_size = 2`, to shrink our dimensions in half.\n",
    "\n",
    "- **Decoder Convolutional Block**: defined by `DecoderConvBlock` class, will be used to increase the dimensionality of our data, as well as perform convolutions. We also define the convolutional layers, but differently than the encoder block, we first upsample our data using `nn.Upsample` with `scale_factor = 2`, because we've used a `nn.MaxPool2d` to decrease the dimensionality in the encoder by a factor of 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels : int, out_channels : int):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.activation = nn.ReLU()\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = self.in_channels, out_channels = self.out_channels, kernel_size = 3, padding = 1)\n",
    "        self.norm1 = nn.BatchNorm2d(self.out_channels)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = self.out_channels, out_channels = self.out_channels, kernel_size = 3, padding = 1)\n",
    "        self.norm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        self.maxpool = nn.MaxPool2d(2, stride = 2, ceil_mode = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv2d_1(x)\n",
    "        out = self.norm1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv2d_2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.maxpool(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "class DecoderConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels : int, out_channels : int, apply_padding : bool = True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "        padding = 1 if apply_padding else 0\n",
    "\n",
    "        self.conv2d_1 = nn.Conv2d(in_channels = self.in_channels, out_channels = self.out_channels, kernel_size = 3, padding = padding)\n",
    "        self.norm1 = nn.BatchNorm2d(self.out_channels)\n",
    "        self.conv2d_2 = nn.Conv2d(in_channels = self.out_channels, out_channels = self.out_channels, kernel_size = 3, padding = 1)\n",
    "        self.norm2 = nn.BatchNorm2d(self.out_channels)\n",
    "        self.upsample = nn.Upsample(scale_factor = 2, mode = 'bilinear', align_corners = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.upsample(x)\n",
    "        out = self.conv2d_1(out)\n",
    "        out = self.norm1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv2d_2(out)\n",
    "        out = self.norm2(out)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Here we define our final model named `CNNAutoEncoder`, which uses a series of encoder convolutional blocks to generate our `bootleneck` and then a series of decoder convolutional blocks to reconstruct the input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNAutoEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.encode1 = EncoderConvBlock(in_channels = 1, out_channels = 32) # 28,28,1 -> 14,14,32\n",
    "        self.encode2 = EncoderConvBlock(in_channels = 32, out_channels = 64) # 14,14,32 -> 7,7,64\n",
    "        self.encode3 = EncoderConvBlock(in_channels = 64, out_channels = 8) # 7,7,64 -> 4,4,8\n",
    "\n",
    "        self.decode3 = DecoderConvBlock(in_channels = 8, out_channels = 64) # 4,4,8 -> 8,8,64\n",
    "        self.decode2 = DecoderConvBlock(in_channels = 64, out_channels = 32) # 8,8,64 -> 16,16,32\n",
    "        self.decode1 = DecoderConvBlock(in_channels = 32, out_channels = 32, apply_padding = False) # 32,32,32\n",
    "\n",
    "        self.output = nn.Sequential(nn.Conv2d(in_channels = 32, out_channels = 1, kernel_size = 3), nn.Sigmoid())\n",
    "\n",
    "        encoders = [self.encode1, self.encode2, self.encode3]\n",
    "        decoders = [self.decode3, self.decode2, self.decode1]\n",
    "        self.encoders = nn.Sequential(*encoders)\n",
    "        self.decoders = nn.Sequential(*decoders)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.encoders(x)\n",
    "        out = self.decoders(out)\n",
    "        out = self.output(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159217"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CNNAutoEncoder().to(device)\n",
    "\n",
    "n_params = sum([p.numel() for p in model.parameters()])\n",
    "n_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training procedure\n",
    "\n",
    "- Differently than the MLP autoencoder, here we will keep things simple and just use the Mean-Squared Error (`nn.MSELoss()`) to define our reconstruction loss function.\n",
    "> Remeber that, when working with autoencoders, we don't have a label to pass to our loss function. Since we want to reconstruct our input data, we pass the original input ($\\mathbf{X}$) as the \"label\" for our loss function and compare it with the reconstructed image ($\\tilde{\\mathbf{X}}$).\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{n}\\sum\\limits_{i=0}^n (\\mathbf{X}_i - \\tilde{\\mathbf{X}_i})^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17e848dba6ce4b879fcf6b9a87ccbd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.00723308976739645, Batch:1/1875, Epoch: 5\n",
      "Loss: 0.005778134800493717, Batch:11/1875, Epoch: 5\n",
      "Loss: 0.005080521106719971, Batch:21/1875, Epoch: 5\n",
      "Loss: 0.005152340512722731, Batch:31/1875, Epoch: 5\n",
      "Loss: 0.007769678253680468, Batch:41/1875, Epoch: 5\n",
      "Loss: 0.005829493515193462, Batch:51/1875, Epoch: 5\n",
      "Loss: 0.006860589142888784, Batch:61/1875, Epoch: 5\n",
      "Loss: 0.005728831980377436, Batch:71/1875, Epoch: 5\n",
      "Loss: 0.00521979620680213, Batch:81/1875, Epoch: 5\n",
      "Loss: 0.005045331548899412, Batch:91/1875, Epoch: 5\n",
      "Loss: 0.004053778480738401, Batch:101/1875, Epoch: 5\n",
      "Loss: 0.004179462790489197, Batch:111/1875, Epoch: 5\n",
      "Loss: 0.004633178003132343, Batch:121/1875, Epoch: 5\n",
      "Loss: 0.003957337234169245, Batch:131/1875, Epoch: 5\n",
      "Loss: 0.00611637718975544, Batch:141/1875, Epoch: 5\n",
      "Loss: 0.005409866571426392, Batch:151/1875, Epoch: 5\n",
      "Loss: 0.004771161358803511, Batch:161/1875, Epoch: 5\n",
      "Loss: 0.0045082056894898415, Batch:171/1875, Epoch: 5\n",
      "Loss: 0.004450768698006868, Batch:181/1875, Epoch: 5\n",
      "Loss: 0.005324744153767824, Batch:191/1875, Epoch: 5\n",
      "Loss: 0.004533222410827875, Batch:201/1875, Epoch: 5\n",
      "Loss: 0.005718579050153494, Batch:211/1875, Epoch: 5\n",
      "Loss: 0.005538280587643385, Batch:221/1875, Epoch: 5\n",
      "Loss: 0.00573076494038105, Batch:231/1875, Epoch: 5\n",
      "Loss: 0.004983577877283096, Batch:241/1875, Epoch: 5\n",
      "Loss: 0.006533566862344742, Batch:251/1875, Epoch: 5\n",
      "Loss: 0.005013932939618826, Batch:261/1875, Epoch: 5\n",
      "Loss: 0.005853069014847279, Batch:271/1875, Epoch: 5\n",
      "Loss: 0.004202812444418669, Batch:281/1875, Epoch: 5\n",
      "Loss: 0.006751741748303175, Batch:291/1875, Epoch: 5\n",
      "Loss: 0.00492607057094574, Batch:301/1875, Epoch: 5\n",
      "Loss: 0.0044171931222081184, Batch:311/1875, Epoch: 5\n",
      "Loss: 0.008384819142520428, Batch:321/1875, Epoch: 5\n",
      "Loss: 0.005228304769843817, Batch:331/1875, Epoch: 5\n",
      "Loss: 0.005129746627062559, Batch:341/1875, Epoch: 5\n",
      "Loss: 0.00476181972771883, Batch:351/1875, Epoch: 5\n",
      "Loss: 0.0041550928726792336, Batch:361/1875, Epoch: 5\n",
      "Loss: 0.004851453006267548, Batch:371/1875, Epoch: 5\n",
      "Loss: 0.004326276946812868, Batch:381/1875, Epoch: 5\n",
      "Loss: 0.004011967685073614, Batch:391/1875, Epoch: 5\n",
      "Loss: 0.004533022176474333, Batch:401/1875, Epoch: 5\n",
      "Loss: 0.005842598620802164, Batch:411/1875, Epoch: 5\n",
      "Loss: 0.004584365524351597, Batch:421/1875, Epoch: 5\n",
      "Loss: 0.0054883151315152645, Batch:431/1875, Epoch: 5\n",
      "Loss: 0.006900065112859011, Batch:441/1875, Epoch: 5\n",
      "Loss: 0.00583933899179101, Batch:451/1875, Epoch: 5\n",
      "Loss: 0.00517638772726059, Batch:461/1875, Epoch: 5\n",
      "Loss: 0.005050784908235073, Batch:471/1875, Epoch: 5\n",
      "Loss: 0.005212746094912291, Batch:481/1875, Epoch: 5\n",
      "Loss: 0.005047168117016554, Batch:491/1875, Epoch: 5\n",
      "Loss: 0.004377839621156454, Batch:501/1875, Epoch: 5\n",
      "Loss: 0.005152157973498106, Batch:511/1875, Epoch: 5\n",
      "Loss: 0.004518201109021902, Batch:521/1875, Epoch: 5\n",
      "Loss: 0.005003215745091438, Batch:531/1875, Epoch: 5\n",
      "Loss: 0.005179328378289938, Batch:541/1875, Epoch: 5\n",
      "Loss: 0.005627814680337906, Batch:551/1875, Epoch: 5\n",
      "Loss: 0.004662021994590759, Batch:561/1875, Epoch: 5\n",
      "Loss: 0.004202220123261213, Batch:571/1875, Epoch: 5\n",
      "Loss: 0.006310980301350355, Batch:581/1875, Epoch: 5\n",
      "Loss: 0.0041868360713124275, Batch:591/1875, Epoch: 5\n",
      "Loss: 0.008484100922942162, Batch:601/1875, Epoch: 5\n",
      "Loss: 0.005335990339517593, Batch:611/1875, Epoch: 5\n",
      "Loss: 0.004270863253623247, Batch:621/1875, Epoch: 5\n",
      "Loss: 0.005459321662783623, Batch:631/1875, Epoch: 5\n",
      "Loss: 0.0037431572563946247, Batch:641/1875, Epoch: 5\n",
      "Loss: 0.003793419571593404, Batch:651/1875, Epoch: 5\n",
      "Loss: 0.004528687801212072, Batch:661/1875, Epoch: 5\n",
      "Loss: 0.0040366617031395435, Batch:671/1875, Epoch: 5\n",
      "Loss: 0.009872430004179478, Batch:681/1875, Epoch: 5\n",
      "Loss: 0.008720929734408855, Batch:691/1875, Epoch: 5\n",
      "Loss: 0.005064033903181553, Batch:701/1875, Epoch: 5\n",
      "Loss: 0.005669707432389259, Batch:711/1875, Epoch: 5\n",
      "Loss: 0.006099109537899494, Batch:721/1875, Epoch: 5\n",
      "Loss: 0.005510244518518448, Batch:731/1875, Epoch: 5\n",
      "Loss: 0.0047935256734490395, Batch:741/1875, Epoch: 5\n",
      "Loss: 0.006062192376703024, Batch:751/1875, Epoch: 5\n",
      "Loss: 0.004722891375422478, Batch:761/1875, Epoch: 5\n",
      "Loss: 0.004982822574675083, Batch:771/1875, Epoch: 5\n",
      "Loss: 0.004064593464136124, Batch:781/1875, Epoch: 5\n",
      "Loss: 0.011482151225209236, Batch:791/1875, Epoch: 5\n",
      "Loss: 0.005403535440564156, Batch:801/1875, Epoch: 5\n",
      "Loss: 0.004795917775481939, Batch:811/1875, Epoch: 5\n",
      "Loss: 0.005255133844912052, Batch:821/1875, Epoch: 5\n",
      "Loss: 0.005126712843775749, Batch:831/1875, Epoch: 5\n",
      "Loss: 0.00498065073043108, Batch:841/1875, Epoch: 5\n",
      "Loss: 0.005907259415835142, Batch:851/1875, Epoch: 5\n",
      "Loss: 0.004544655326753855, Batch:861/1875, Epoch: 5\n",
      "Loss: 0.004237453918904066, Batch:871/1875, Epoch: 5\n",
      "Loss: 0.00513483677059412, Batch:881/1875, Epoch: 5\n",
      "Loss: 0.0041823117062449455, Batch:891/1875, Epoch: 5\n",
      "Loss: 0.004341344814747572, Batch:901/1875, Epoch: 5\n",
      "Loss: 0.006205652374774218, Batch:911/1875, Epoch: 5\n",
      "Loss: 0.00480628339573741, Batch:921/1875, Epoch: 5\n",
      "Loss: 0.005296854302287102, Batch:931/1875, Epoch: 5\n",
      "Loss: 0.0049200719222426414, Batch:941/1875, Epoch: 5\n",
      "Loss: 0.004593128338456154, Batch:951/1875, Epoch: 5\n",
      "Loss: 0.005698870401829481, Batch:961/1875, Epoch: 5\n",
      "Loss: 0.005527017172425985, Batch:971/1875, Epoch: 5\n",
      "Loss: 0.004235865082591772, Batch:981/1875, Epoch: 5\n",
      "Loss: 0.0057605295442044735, Batch:991/1875, Epoch: 5\n",
      "Loss: 0.0056959898211061954, Batch:1001/1875, Epoch: 5\n",
      "Loss: 0.0056043765507638454, Batch:1011/1875, Epoch: 5\n",
      "Loss: 0.005801503546535969, Batch:1021/1875, Epoch: 5\n",
      "Loss: 0.0043108221143484116, Batch:1031/1875, Epoch: 5\n",
      "Loss: 0.0059428769163787365, Batch:1041/1875, Epoch: 5\n",
      "Loss: 0.005166536662727594, Batch:1051/1875, Epoch: 5\n",
      "Loss: 0.0052059609442949295, Batch:1061/1875, Epoch: 5\n",
      "Loss: 0.0038804695941507816, Batch:1071/1875, Epoch: 5\n",
      "Loss: 0.004421357065439224, Batch:1081/1875, Epoch: 5\n",
      "Loss: 0.004459593445062637, Batch:1091/1875, Epoch: 5\n",
      "Loss: 0.004745342303067446, Batch:1101/1875, Epoch: 5\n",
      "Loss: 0.004640784114599228, Batch:1111/1875, Epoch: 5\n",
      "Loss: 0.004355425480753183, Batch:1121/1875, Epoch: 5\n",
      "Loss: 0.011086233891546726, Batch:1131/1875, Epoch: 5\n",
      "Loss: 0.007171567529439926, Batch:1141/1875, Epoch: 5\n",
      "Loss: 0.005562518257647753, Batch:1151/1875, Epoch: 5\n",
      "Loss: 0.005435188300907612, Batch:1161/1875, Epoch: 5\n",
      "Loss: 0.006407223176211119, Batch:1171/1875, Epoch: 5\n",
      "Loss: 0.007242878898978233, Batch:1181/1875, Epoch: 5\n",
      "Loss: 0.005540656857192516, Batch:1191/1875, Epoch: 5\n",
      "Loss: 0.004437904339283705, Batch:1201/1875, Epoch: 5\n",
      "Loss: 0.005241176579147577, Batch:1211/1875, Epoch: 5\n",
      "Loss: 0.004084030631929636, Batch:1221/1875, Epoch: 5\n",
      "Loss: 0.00462630670517683, Batch:1231/1875, Epoch: 5\n",
      "Loss: 0.00395233603194356, Batch:1241/1875, Epoch: 5\n",
      "Loss: 0.006598592270165682, Batch:1251/1875, Epoch: 5\n",
      "Loss: 0.005217669066041708, Batch:1261/1875, Epoch: 5\n",
      "Loss: 0.0048703090287745, Batch:1271/1875, Epoch: 5\n",
      "Loss: 0.004297392908483744, Batch:1281/1875, Epoch: 5\n",
      "Loss: 0.006353241857141256, Batch:1291/1875, Epoch: 5\n",
      "Loss: 0.005000193137675524, Batch:1301/1875, Epoch: 5\n",
      "Loss: 0.00508938217535615, Batch:1311/1875, Epoch: 5\n",
      "Loss: 0.0047382484190166, Batch:1321/1875, Epoch: 5\n",
      "Loss: 0.0071114604361355305, Batch:1331/1875, Epoch: 5\n",
      "Loss: 0.006013606209307909, Batch:1341/1875, Epoch: 5\n",
      "Loss: 0.00502867391332984, Batch:1351/1875, Epoch: 5\n",
      "Loss: 0.004810062702745199, Batch:1361/1875, Epoch: 5\n",
      "Loss: 0.005824103485792875, Batch:1371/1875, Epoch: 5\n",
      "Loss: 0.005135087762027979, Batch:1381/1875, Epoch: 5\n",
      "Loss: 0.004175227601081133, Batch:1391/1875, Epoch: 5\n",
      "Loss: 0.00507119856774807, Batch:1401/1875, Epoch: 5\n",
      "Loss: 0.004512612707912922, Batch:1411/1875, Epoch: 5\n",
      "Loss: 0.005105005111545324, Batch:1421/1875, Epoch: 5\n",
      "Loss: 0.005536083597689867, Batch:1431/1875, Epoch: 5\n",
      "Loss: 0.004424627870321274, Batch:1441/1875, Epoch: 5\n",
      "Loss: 0.004899824969470501, Batch:1451/1875, Epoch: 5\n",
      "Loss: 0.004283498506993055, Batch:1461/1875, Epoch: 5\n",
      "Loss: 0.0037693320773541927, Batch:1471/1875, Epoch: 5\n",
      "Loss: 0.0058669052086770535, Batch:1481/1875, Epoch: 5\n",
      "Loss: 0.005331117194145918, Batch:1491/1875, Epoch: 5\n",
      "Loss: 0.005368177779018879, Batch:1501/1875, Epoch: 5\n",
      "Loss: 0.004833905026316643, Batch:1511/1875, Epoch: 5\n",
      "Loss: 0.0038212956860661507, Batch:1521/1875, Epoch: 5\n",
      "Loss: 0.006216831039637327, Batch:1531/1875, Epoch: 5\n",
      "Loss: 0.004549839999526739, Batch:1541/1875, Epoch: 5\n",
      "Loss: 0.003973638638854027, Batch:1551/1875, Epoch: 5\n",
      "Loss: 0.004436270333826542, Batch:1561/1875, Epoch: 5\n",
      "Loss: 0.0042454395443201065, Batch:1571/1875, Epoch: 5\n",
      "Loss: 0.004378274083137512, Batch:1581/1875, Epoch: 5\n",
      "Loss: 0.00473712058737874, Batch:1591/1875, Epoch: 5\n",
      "Loss: 0.005595223046839237, Batch:1601/1875, Epoch: 5\n",
      "Loss: 0.0039030564948916435, Batch:1611/1875, Epoch: 5\n",
      "Loss: 0.0052676270715892315, Batch:1621/1875, Epoch: 5\n",
      "Loss: 0.004150291904807091, Batch:1631/1875, Epoch: 5\n",
      "Loss: 0.0037552479188889265, Batch:1641/1875, Epoch: 5\n",
      "Loss: 0.0037912586703896523, Batch:1651/1875, Epoch: 5\n",
      "Loss: 0.003951994702219963, Batch:1661/1875, Epoch: 5\n",
      "Loss: 0.0037749868351966143, Batch:1671/1875, Epoch: 5\n",
      "Loss: 0.0036780836526304483, Batch:1681/1875, Epoch: 5\n",
      "Loss: 0.004859762266278267, Batch:1691/1875, Epoch: 5\n",
      "Loss: 0.0042297253385186195, Batch:1701/1875, Epoch: 5\n",
      "Loss: 0.004224181640893221, Batch:1711/1875, Epoch: 5\n",
      "Loss: 0.005055634304881096, Batch:1721/1875, Epoch: 5\n",
      "Loss: 0.0035451683215796947, Batch:1731/1875, Epoch: 5\n",
      "Loss: 0.004358795937150717, Batch:1741/1875, Epoch: 5\n",
      "Loss: 0.004534961190074682, Batch:1751/1875, Epoch: 5\n",
      "Loss: 0.0045404937118291855, Batch:1761/1875, Epoch: 5\n",
      "Loss: 0.005331052001565695, Batch:1771/1875, Epoch: 5\n",
      "Loss: 0.004716221708804369, Batch:1781/1875, Epoch: 5\n",
      "Loss: 0.004948154091835022, Batch:1791/1875, Epoch: 5\n",
      "Loss: 0.004400738049298525, Batch:1801/1875, Epoch: 5\n",
      "Loss: 0.00449243513867259, Batch:1811/1875, Epoch: 5\n",
      "Loss: 0.004896940663456917, Batch:1821/1875, Epoch: 5\n",
      "Loss: 0.006305846385657787, Batch:1831/1875, Epoch: 5\n",
      "Loss: 0.006960250902920961, Batch:1841/1875, Epoch: 5\n",
      "Loss: 0.0049603888764977455, Batch:1851/1875, Epoch: 5\n",
      "Loss: 0.012391146272420883, Batch:1861/1875, Epoch: 5\n",
      "Loss: 0.007417828310281038, Batch:1871/1875, Epoch: 5\n",
      "Loss: 0.005052939057350159, Batch:1/1875, Epoch: 10\n",
      "Loss: 0.0040574995800852776, Batch:11/1875, Epoch: 10\n",
      "Loss: 0.004032991360872984, Batch:21/1875, Epoch: 10\n",
      "Loss: 0.003645518561825156, Batch:31/1875, Epoch: 10\n",
      "Loss: 0.004292917903512716, Batch:41/1875, Epoch: 10\n",
      "Loss: 0.003717069746926427, Batch:51/1875, Epoch: 10\n",
      "Loss: 0.0036768997088074684, Batch:61/1875, Epoch: 10\n",
      "Loss: 0.0033561806194484234, Batch:71/1875, Epoch: 10\n",
      "Loss: 0.003018053248524666, Batch:81/1875, Epoch: 10\n",
      "Loss: 0.003706500167027116, Batch:91/1875, Epoch: 10\n",
      "Loss: 0.002890398260205984, Batch:101/1875, Epoch: 10\n",
      "Loss: 0.002994807902723551, Batch:111/1875, Epoch: 10\n",
      "Loss: 0.0033367567230015993, Batch:121/1875, Epoch: 10\n",
      "Loss: 0.0027754774782806635, Batch:131/1875, Epoch: 10\n",
      "Loss: 0.003750875825062394, Batch:141/1875, Epoch: 10\n",
      "Loss: 0.0038739433512091637, Batch:151/1875, Epoch: 10\n",
      "Loss: 0.0036033999640494585, Batch:161/1875, Epoch: 10\n",
      "Loss: 0.00324193830601871, Batch:171/1875, Epoch: 10\n",
      "Loss: 0.0030681758653372526, Batch:181/1875, Epoch: 10\n",
      "Loss: 0.003490220056846738, Batch:191/1875, Epoch: 10\n",
      "Loss: 0.0030781510286033154, Batch:201/1875, Epoch: 10\n",
      "Loss: 0.004084611777216196, Batch:211/1875, Epoch: 10\n",
      "Loss: 0.0040323263965547085, Batch:221/1875, Epoch: 10\n",
      "Loss: 0.0036210045218467712, Batch:231/1875, Epoch: 10\n",
      "Loss: 0.0031028520315885544, Batch:241/1875, Epoch: 10\n",
      "Loss: 0.004556887783110142, Batch:251/1875, Epoch: 10\n",
      "Loss: 0.0032698793802410364, Batch:261/1875, Epoch: 10\n",
      "Loss: 0.003477911464869976, Batch:271/1875, Epoch: 10\n",
      "Loss: 0.002690556924790144, Batch:281/1875, Epoch: 10\n",
      "Loss: 0.00439284835010767, Batch:291/1875, Epoch: 10\n",
      "Loss: 0.003379665780812502, Batch:301/1875, Epoch: 10\n",
      "Loss: 0.0030892661307007074, Batch:311/1875, Epoch: 10\n",
      "Loss: 0.0043081194162368774, Batch:321/1875, Epoch: 10\n",
      "Loss: 0.0031515073496848345, Batch:331/1875, Epoch: 10\n",
      "Loss: 0.002984050428494811, Batch:341/1875, Epoch: 10\n",
      "Loss: 0.0030182087793946266, Batch:351/1875, Epoch: 10\n",
      "Loss: 0.003178550163283944, Batch:361/1875, Epoch: 10\n",
      "Loss: 0.0033702882938086987, Batch:371/1875, Epoch: 10\n",
      "Loss: 0.003133474150672555, Batch:381/1875, Epoch: 10\n",
      "Loss: 0.003108987584710121, Batch:391/1875, Epoch: 10\n",
      "Loss: 0.0029929978772997856, Batch:401/1875, Epoch: 10\n",
      "Loss: 0.0041988021694123745, Batch:411/1875, Epoch: 10\n",
      "Loss: 0.0033261373173445463, Batch:421/1875, Epoch: 10\n",
      "Loss: 0.004043388646095991, Batch:431/1875, Epoch: 10\n",
      "Loss: 0.003476959653198719, Batch:441/1875, Epoch: 10\n",
      "Loss: 0.00377692305482924, Batch:451/1875, Epoch: 10\n",
      "Loss: 0.003192298812791705, Batch:461/1875, Epoch: 10\n",
      "Loss: 0.0034785072784870863, Batch:471/1875, Epoch: 10\n",
      "Loss: 0.003484489396214485, Batch:481/1875, Epoch: 10\n",
      "Loss: 0.0038948412984609604, Batch:491/1875, Epoch: 10\n",
      "Loss: 0.002882677596062422, Batch:501/1875, Epoch: 10\n",
      "Loss: 0.0032013265881687403, Batch:511/1875, Epoch: 10\n",
      "Loss: 0.003013178240507841, Batch:521/1875, Epoch: 10\n",
      "Loss: 0.003345037577673793, Batch:531/1875, Epoch: 10\n",
      "Loss: 0.003247685730457306, Batch:541/1875, Epoch: 10\n",
      "Loss: 0.003901830641552806, Batch:551/1875, Epoch: 10\n",
      "Loss: 0.003013135399669409, Batch:561/1875, Epoch: 10\n",
      "Loss: 0.002890265081077814, Batch:571/1875, Epoch: 10\n",
      "Loss: 0.004418840631842613, Batch:581/1875, Epoch: 10\n",
      "Loss: 0.003001172561198473, Batch:591/1875, Epoch: 10\n",
      "Loss: 0.004731128923594952, Batch:601/1875, Epoch: 10\n",
      "Loss: 0.0035696052946150303, Batch:611/1875, Epoch: 10\n",
      "Loss: 0.0030867508612573147, Batch:621/1875, Epoch: 10\n",
      "Loss: 0.003722282126545906, Batch:631/1875, Epoch: 10\n",
      "Loss: 0.0026203864254057407, Batch:641/1875, Epoch: 10\n",
      "Loss: 0.0027245082892477512, Batch:651/1875, Epoch: 10\n",
      "Loss: 0.003305265214294195, Batch:661/1875, Epoch: 10\n",
      "Loss: 0.0027740015648305416, Batch:671/1875, Epoch: 10\n",
      "Loss: 0.006004814989864826, Batch:681/1875, Epoch: 10\n",
      "Loss: 0.004265220370143652, Batch:691/1875, Epoch: 10\n",
      "Loss: 0.0032090514432638884, Batch:701/1875, Epoch: 10\n",
      "Loss: 0.0038294976111501455, Batch:711/1875, Epoch: 10\n",
      "Loss: 0.004482772666960955, Batch:721/1875, Epoch: 10\n",
      "Loss: 0.0038299639709293842, Batch:731/1875, Epoch: 10\n",
      "Loss: 0.0031754975207149982, Batch:741/1875, Epoch: 10\n",
      "Loss: 0.0035652657970786095, Batch:751/1875, Epoch: 10\n",
      "Loss: 0.002977901604026556, Batch:761/1875, Epoch: 10\n",
      "Loss: 0.00326422112993896, Batch:771/1875, Epoch: 10\n",
      "Loss: 0.002626350848004222, Batch:781/1875, Epoch: 10\n",
      "Loss: 0.005491769872605801, Batch:791/1875, Epoch: 10\n",
      "Loss: 0.0030452432110905647, Batch:801/1875, Epoch: 10\n",
      "Loss: 0.0031765655148774385, Batch:811/1875, Epoch: 10\n",
      "Loss: 0.003421905916184187, Batch:821/1875, Epoch: 10\n",
      "Loss: 0.004136854317039251, Batch:831/1875, Epoch: 10\n",
      "Loss: 0.003457127371802926, Batch:841/1875, Epoch: 10\n",
      "Loss: 0.003865327686071396, Batch:851/1875, Epoch: 10\n",
      "Loss: 0.003139858366921544, Batch:861/1875, Epoch: 10\n",
      "Loss: 0.003223741427063942, Batch:871/1875, Epoch: 10\n",
      "Loss: 0.0033559519797563553, Batch:881/1875, Epoch: 10\n",
      "Loss: 0.0031559409108012915, Batch:891/1875, Epoch: 10\n",
      "Loss: 0.0031411312520503998, Batch:901/1875, Epoch: 10\n",
      "Loss: 0.0039849309250712395, Batch:911/1875, Epoch: 10\n",
      "Loss: 0.0031760234851390123, Batch:921/1875, Epoch: 10\n",
      "Loss: 0.0034038922749459743, Batch:931/1875, Epoch: 10\n",
      "Loss: 0.003648479236289859, Batch:941/1875, Epoch: 10\n",
      "Loss: 0.0032956728246062994, Batch:951/1875, Epoch: 10\n",
      "Loss: 0.0036628295201808214, Batch:961/1875, Epoch: 10\n",
      "Loss: 0.003981074783951044, Batch:971/1875, Epoch: 10\n",
      "Loss: 0.0028166347183287144, Batch:981/1875, Epoch: 10\n",
      "Loss: 0.004341984633356333, Batch:991/1875, Epoch: 10\n",
      "Loss: 0.0037978235632181168, Batch:1001/1875, Epoch: 10\n",
      "Loss: 0.0039982181042432785, Batch:1011/1875, Epoch: 10\n",
      "Loss: 0.0034018359147012234, Batch:1021/1875, Epoch: 10\n",
      "Loss: 0.003614419838413596, Batch:1031/1875, Epoch: 10\n",
      "Loss: 0.0036704125814139843, Batch:1041/1875, Epoch: 10\n",
      "Loss: 0.0035254131071269512, Batch:1051/1875, Epoch: 10\n",
      "Loss: 0.003869384527206421, Batch:1061/1875, Epoch: 10\n",
      "Loss: 0.0030225226655602455, Batch:1071/1875, Epoch: 10\n",
      "Loss: 0.0031210719607770443, Batch:1081/1875, Epoch: 10\n",
      "Loss: 0.0032193742226809263, Batch:1091/1875, Epoch: 10\n",
      "Loss: 0.0035675500985234976, Batch:1101/1875, Epoch: 10\n",
      "Loss: 0.003574890084564686, Batch:1111/1875, Epoch: 10\n",
      "Loss: 0.0031188202556222677, Batch:1121/1875, Epoch: 10\n",
      "Loss: 0.00578761100769043, Batch:1131/1875, Epoch: 10\n",
      "Loss: 0.0042268442921340466, Batch:1141/1875, Epoch: 10\n",
      "Loss: 0.0045512160286307335, Batch:1151/1875, Epoch: 10\n",
      "Loss: 0.004239789675921202, Batch:1161/1875, Epoch: 10\n",
      "Loss: 0.0044151535257697105, Batch:1171/1875, Epoch: 10\n",
      "Loss: 0.004896904807537794, Batch:1181/1875, Epoch: 10\n",
      "Loss: 0.003895784495398402, Batch:1191/1875, Epoch: 10\n",
      "Loss: 0.0035870245192199945, Batch:1201/1875, Epoch: 10\n",
      "Loss: 0.003652696032077074, Batch:1211/1875, Epoch: 10\n",
      "Loss: 0.0028497003950178623, Batch:1221/1875, Epoch: 10\n",
      "Loss: 0.00321567477658391, Batch:1231/1875, Epoch: 10\n",
      "Loss: 0.0027200302574783564, Batch:1241/1875, Epoch: 10\n",
      "Loss: 0.0038242044392973185, Batch:1251/1875, Epoch: 10\n",
      "Loss: 0.0028514836449176073, Batch:1261/1875, Epoch: 10\n",
      "Loss: 0.003606240963563323, Batch:1271/1875, Epoch: 10\n",
      "Loss: 0.003478304948657751, Batch:1281/1875, Epoch: 10\n",
      "Loss: 0.004166068509221077, Batch:1291/1875, Epoch: 10\n",
      "Loss: 0.0034175936598330736, Batch:1301/1875, Epoch: 10\n",
      "Loss: 0.003308844519779086, Batch:1311/1875, Epoch: 10\n",
      "Loss: 0.003284980310127139, Batch:1321/1875, Epoch: 10\n",
      "Loss: 0.003931288607418537, Batch:1331/1875, Epoch: 10\n",
      "Loss: 0.0031609968282282352, Batch:1341/1875, Epoch: 10\n",
      "Loss: 0.003287665080279112, Batch:1351/1875, Epoch: 10\n",
      "Loss: 0.003172273514792323, Batch:1361/1875, Epoch: 10\n",
      "Loss: 0.004325503017753363, Batch:1371/1875, Epoch: 10\n",
      "Loss: 0.003755169687792659, Batch:1381/1875, Epoch: 10\n",
      "Loss: 0.0027977318968623877, Batch:1391/1875, Epoch: 10\n",
      "Loss: 0.0032164326403290033, Batch:1401/1875, Epoch: 10\n",
      "Loss: 0.0033755868207663298, Batch:1411/1875, Epoch: 10\n",
      "Loss: 0.0033870621118694544, Batch:1421/1875, Epoch: 10\n",
      "Loss: 0.004269161261618137, Batch:1431/1875, Epoch: 10\n",
      "Loss: 0.0032147704623639584, Batch:1441/1875, Epoch: 10\n",
      "Loss: 0.003693572711199522, Batch:1451/1875, Epoch: 10\n",
      "Loss: 0.0029970118775963783, Batch:1461/1875, Epoch: 10\n",
      "Loss: 0.00282340869307518, Batch:1471/1875, Epoch: 10\n",
      "Loss: 0.004015575163066387, Batch:1481/1875, Epoch: 10\n",
      "Loss: 0.0038081202656030655, Batch:1491/1875, Epoch: 10\n",
      "Loss: 0.003784662578254938, Batch:1501/1875, Epoch: 10\n",
      "Loss: 0.0033859526738524437, Batch:1511/1875, Epoch: 10\n",
      "Loss: 0.002692159963771701, Batch:1521/1875, Epoch: 10\n",
      "Loss: 0.003926177974790335, Batch:1531/1875, Epoch: 10\n",
      "Loss: 0.0033312670420855284, Batch:1541/1875, Epoch: 10\n",
      "Loss: 0.0031727843452244997, Batch:1551/1875, Epoch: 10\n",
      "Loss: 0.003348857630044222, Batch:1561/1875, Epoch: 10\n",
      "Loss: 0.0029224553145468235, Batch:1571/1875, Epoch: 10\n",
      "Loss: 0.003112947102636099, Batch:1581/1875, Epoch: 10\n",
      "Loss: 0.003280903911218047, Batch:1591/1875, Epoch: 10\n",
      "Loss: 0.0037854318507015705, Batch:1601/1875, Epoch: 10\n",
      "Loss: 0.0025428729131817818, Batch:1611/1875, Epoch: 10\n",
      "Loss: 0.003952994477003813, Batch:1621/1875, Epoch: 10\n",
      "Loss: 0.0029889915604144335, Batch:1631/1875, Epoch: 10\n",
      "Loss: 0.0025738468393683434, Batch:1641/1875, Epoch: 10\n",
      "Loss: 0.0029027904383838177, Batch:1651/1875, Epoch: 10\n",
      "Loss: 0.0027721186634153128, Batch:1661/1875, Epoch: 10\n",
      "Loss: 0.0029425928369164467, Batch:1671/1875, Epoch: 10\n",
      "Loss: 0.002587235067039728, Batch:1681/1875, Epoch: 10\n",
      "Loss: 0.003928140737116337, Batch:1691/1875, Epoch: 10\n",
      "Loss: 0.002871663309633732, Batch:1701/1875, Epoch: 10\n",
      "Loss: 0.0030356179922819138, Batch:1711/1875, Epoch: 10\n",
      "Loss: 0.003577257739380002, Batch:1721/1875, Epoch: 10\n",
      "Loss: 0.0025725336745381355, Batch:1731/1875, Epoch: 10\n",
      "Loss: 0.0031376115512102842, Batch:1741/1875, Epoch: 10\n",
      "Loss: 0.0031207927968353033, Batch:1751/1875, Epoch: 10\n",
      "Loss: 0.003246284555643797, Batch:1761/1875, Epoch: 10\n",
      "Loss: 0.003765137167647481, Batch:1771/1875, Epoch: 10\n",
      "Loss: 0.003021841635927558, Batch:1781/1875, Epoch: 10\n",
      "Loss: 0.0031076171435415745, Batch:1791/1875, Epoch: 10\n",
      "Loss: 0.002855295082554221, Batch:1801/1875, Epoch: 10\n",
      "Loss: 0.003081081435084343, Batch:1811/1875, Epoch: 10\n",
      "Loss: 0.0037629103753715754, Batch:1821/1875, Epoch: 10\n",
      "Loss: 0.0036587307695299387, Batch:1831/1875, Epoch: 10\n",
      "Loss: 0.0043296730145812035, Batch:1841/1875, Epoch: 10\n",
      "Loss: 0.0038415424060076475, Batch:1851/1875, Epoch: 10\n",
      "Loss: 0.007666895631700754, Batch:1861/1875, Epoch: 10\n",
      "Loss: 0.005587617866694927, Batch:1871/1875, Epoch: 10\n",
      "Loss: 0.004233970306813717, Batch:1/1875, Epoch: 15\n",
      "Loss: 0.0034899290185421705, Batch:11/1875, Epoch: 15\n",
      "Loss: 0.003400449175387621, Batch:21/1875, Epoch: 15\n",
      "Loss: 0.002596306847408414, Batch:31/1875, Epoch: 15\n",
      "Loss: 0.0032376968301832676, Batch:41/1875, Epoch: 15\n",
      "Loss: 0.0031593404710292816, Batch:51/1875, Epoch: 15\n",
      "Loss: 0.003060890594497323, Batch:61/1875, Epoch: 15\n",
      "Loss: 0.0026803435757756233, Batch:71/1875, Epoch: 15\n",
      "Loss: 0.0024112272076308727, Batch:81/1875, Epoch: 15\n",
      "Loss: 0.0032282297033816576, Batch:91/1875, Epoch: 15\n",
      "Loss: 0.0022647364530712366, Batch:101/1875, Epoch: 15\n",
      "Loss: 0.002210813807323575, Batch:111/1875, Epoch: 15\n",
      "Loss: 0.0026116473600268364, Batch:121/1875, Epoch: 15\n",
      "Loss: 0.0022909322287887335, Batch:131/1875, Epoch: 15\n",
      "Loss: 0.0029659667052328587, Batch:141/1875, Epoch: 15\n",
      "Loss: 0.0031699452083557844, Batch:151/1875, Epoch: 15\n",
      "Loss: 0.0033631320111453533, Batch:161/1875, Epoch: 15\n",
      "Loss: 0.0026822437066584826, Batch:171/1875, Epoch: 15\n",
      "Loss: 0.002675749361515045, Batch:181/1875, Epoch: 15\n",
      "Loss: 0.003006411949172616, Batch:191/1875, Epoch: 15\n",
      "Loss: 0.0024223257787525654, Batch:201/1875, Epoch: 15\n",
      "Loss: 0.003396780462935567, Batch:211/1875, Epoch: 15\n",
      "Loss: 0.0031267227604985237, Batch:221/1875, Epoch: 15\n",
      "Loss: 0.0031001896131783724, Batch:231/1875, Epoch: 15\n",
      "Loss: 0.0024138963781297207, Batch:241/1875, Epoch: 15\n",
      "Loss: 0.0034586817491799593, Batch:251/1875, Epoch: 15\n",
      "Loss: 0.0026552171912044287, Batch:261/1875, Epoch: 15\n",
      "Loss: 0.0026070738676935434, Batch:271/1875, Epoch: 15\n",
      "Loss: 0.0021892874501645565, Batch:281/1875, Epoch: 15\n",
      "Loss: 0.0033982298336923122, Batch:291/1875, Epoch: 15\n",
      "Loss: 0.002798999659717083, Batch:301/1875, Epoch: 15\n",
      "Loss: 0.002651061164215207, Batch:311/1875, Epoch: 15\n",
      "Loss: 0.0033043704461306334, Batch:321/1875, Epoch: 15\n",
      "Loss: 0.0026711304672062397, Batch:331/1875, Epoch: 15\n",
      "Loss: 0.0022893508430570364, Batch:341/1875, Epoch: 15\n",
      "Loss: 0.0026961113326251507, Batch:351/1875, Epoch: 15\n",
      "Loss: 0.002557608298957348, Batch:361/1875, Epoch: 15\n",
      "Loss: 0.002773116109892726, Batch:371/1875, Epoch: 15\n",
      "Loss: 0.0025360281579196453, Batch:381/1875, Epoch: 15\n",
      "Loss: 0.0023953616619110107, Batch:391/1875, Epoch: 15\n",
      "Loss: 0.0024688902776688337, Batch:401/1875, Epoch: 15\n",
      "Loss: 0.0034553389996290207, Batch:411/1875, Epoch: 15\n",
      "Loss: 0.002513290848582983, Batch:421/1875, Epoch: 15\n",
      "Loss: 0.0035034988541156054, Batch:431/1875, Epoch: 15\n",
      "Loss: 0.0029813083820044994, Batch:441/1875, Epoch: 15\n",
      "Loss: 0.0028654872439801693, Batch:451/1875, Epoch: 15\n",
      "Loss: 0.0028323493897914886, Batch:461/1875, Epoch: 15\n",
      "Loss: 0.002684985985979438, Batch:471/1875, Epoch: 15\n",
      "Loss: 0.002695112256333232, Batch:481/1875, Epoch: 15\n",
      "Loss: 0.0032652183435857296, Batch:491/1875, Epoch: 15\n",
      "Loss: 0.002378116361796856, Batch:501/1875, Epoch: 15\n",
      "Loss: 0.0026529442984610796, Batch:511/1875, Epoch: 15\n",
      "Loss: 0.0026644242461770773, Batch:521/1875, Epoch: 15\n",
      "Loss: 0.0026006202679127455, Batch:531/1875, Epoch: 15\n",
      "Loss: 0.00256382510997355, Batch:541/1875, Epoch: 15\n",
      "Loss: 0.003254082053899765, Batch:551/1875, Epoch: 15\n",
      "Loss: 0.0025320714339613914, Batch:561/1875, Epoch: 15\n",
      "Loss: 0.002419376512989402, Batch:571/1875, Epoch: 15\n",
      "Loss: 0.0033904584124684334, Batch:581/1875, Epoch: 15\n",
      "Loss: 0.002449217950925231, Batch:591/1875, Epoch: 15\n",
      "Loss: 0.0036168743390589952, Batch:601/1875, Epoch: 15\n",
      "Loss: 0.0028624930419027805, Batch:611/1875, Epoch: 15\n",
      "Loss: 0.002635336946696043, Batch:621/1875, Epoch: 15\n",
      "Loss: 0.002881094813346863, Batch:631/1875, Epoch: 15\n",
      "Loss: 0.0021048227790743113, Batch:641/1875, Epoch: 15\n",
      "Loss: 0.002383116167038679, Batch:651/1875, Epoch: 15\n",
      "Loss: 0.0027719333302229643, Batch:661/1875, Epoch: 15\n",
      "Loss: 0.0022715209051966667, Batch:671/1875, Epoch: 15\n",
      "Loss: 0.004353540018200874, Batch:681/1875, Epoch: 15\n",
      "Loss: 0.0032619258854538202, Batch:691/1875, Epoch: 15\n",
      "Loss: 0.0027069502975791693, Batch:701/1875, Epoch: 15\n",
      "Loss: 0.003140578046441078, Batch:711/1875, Epoch: 15\n",
      "Loss: 0.003866905579343438, Batch:721/1875, Epoch: 15\n",
      "Loss: 0.0030320854857563972, Batch:731/1875, Epoch: 15\n",
      "Loss: 0.0027590065728873014, Batch:741/1875, Epoch: 15\n",
      "Loss: 0.0029307669028639793, Batch:751/1875, Epoch: 15\n",
      "Loss: 0.0022687928285449743, Batch:761/1875, Epoch: 15\n",
      "Loss: 0.0025607587303966284, Batch:771/1875, Epoch: 15\n",
      "Loss: 0.0021787656005471945, Batch:781/1875, Epoch: 15\n",
      "Loss: 0.003970068413764238, Batch:791/1875, Epoch: 15\n",
      "Loss: 0.0025309929624199867, Batch:801/1875, Epoch: 15\n",
      "Loss: 0.0025843509938567877, Batch:811/1875, Epoch: 15\n",
      "Loss: 0.002713075140491128, Batch:821/1875, Epoch: 15\n",
      "Loss: 0.002998345997184515, Batch:831/1875, Epoch: 15\n",
      "Loss: 0.0026439670473337173, Batch:841/1875, Epoch: 15\n",
      "Loss: 0.003097787033766508, Batch:851/1875, Epoch: 15\n",
      "Loss: 0.002566744340583682, Batch:861/1875, Epoch: 15\n",
      "Loss: 0.0026467565912753344, Batch:871/1875, Epoch: 15\n",
      "Loss: 0.002708702115342021, Batch:881/1875, Epoch: 15\n",
      "Loss: 0.002605913206934929, Batch:891/1875, Epoch: 15\n",
      "Loss: 0.002544211922213435, Batch:901/1875, Epoch: 15\n",
      "Loss: 0.003300353419035673, Batch:911/1875, Epoch: 15\n",
      "Loss: 0.002565424656495452, Batch:921/1875, Epoch: 15\n",
      "Loss: 0.002888529561460018, Batch:931/1875, Epoch: 15\n",
      "Loss: 0.0031500516925007105, Batch:941/1875, Epoch: 15\n",
      "Loss: 0.002896744292229414, Batch:951/1875, Epoch: 15\n",
      "Loss: 0.0030240321066230536, Batch:961/1875, Epoch: 15\n",
      "Loss: 0.0034572372678667307, Batch:971/1875, Epoch: 15\n",
      "Loss: 0.002517255023121834, Batch:981/1875, Epoch: 15\n",
      "Loss: 0.003490022150799632, Batch:991/1875, Epoch: 15\n",
      "Loss: 0.0031994516029953957, Batch:1001/1875, Epoch: 15\n",
      "Loss: 0.003258661599829793, Batch:1011/1875, Epoch: 15\n",
      "Loss: 0.002678296994417906, Batch:1021/1875, Epoch: 15\n",
      "Loss: 0.0029899387154728174, Batch:1031/1875, Epoch: 15\n",
      "Loss: 0.0030102520249783993, Batch:1041/1875, Epoch: 15\n",
      "Loss: 0.0028887668158859015, Batch:1051/1875, Epoch: 15\n",
      "Loss: 0.0031547406688332558, Batch:1061/1875, Epoch: 15\n",
      "Loss: 0.0025682204868644476, Batch:1071/1875, Epoch: 15\n",
      "Loss: 0.0027470996137708426, Batch:1081/1875, Epoch: 15\n",
      "Loss: 0.002682062331587076, Batch:1091/1875, Epoch: 15\n",
      "Loss: 0.002930685179308057, Batch:1101/1875, Epoch: 15\n",
      "Loss: 0.0032928406726568937, Batch:1111/1875, Epoch: 15\n",
      "Loss: 0.0026648431085050106, Batch:1121/1875, Epoch: 15\n",
      "Loss: 0.0041830409318208694, Batch:1131/1875, Epoch: 15\n",
      "Loss: 0.0038033626042306423, Batch:1141/1875, Epoch: 15\n",
      "Loss: 0.003959016874432564, Batch:1151/1875, Epoch: 15\n",
      "Loss: 0.0034576610196381807, Batch:1161/1875, Epoch: 15\n",
      "Loss: 0.0036031834315508604, Batch:1171/1875, Epoch: 15\n",
      "Loss: 0.0038474486209452152, Batch:1181/1875, Epoch: 15\n",
      "Loss: 0.0034385165199637413, Batch:1191/1875, Epoch: 15\n",
      "Loss: 0.0028997519984841347, Batch:1201/1875, Epoch: 15\n",
      "Loss: 0.003016411093994975, Batch:1211/1875, Epoch: 15\n",
      "Loss: 0.0023926105350255966, Batch:1221/1875, Epoch: 15\n",
      "Loss: 0.002565068891271949, Batch:1231/1875, Epoch: 15\n",
      "Loss: 0.002415011404082179, Batch:1241/1875, Epoch: 15\n",
      "Loss: 0.0031483054626733065, Batch:1251/1875, Epoch: 15\n",
      "Loss: 0.0023797801695764065, Batch:1261/1875, Epoch: 15\n",
      "Loss: 0.003128882497549057, Batch:1271/1875, Epoch: 15\n",
      "Loss: 0.0029201596044003963, Batch:1281/1875, Epoch: 15\n",
      "Loss: 0.0033774953335523605, Batch:1291/1875, Epoch: 15\n",
      "Loss: 0.00284362374804914, Batch:1301/1875, Epoch: 15\n",
      "Loss: 0.002863374538719654, Batch:1311/1875, Epoch: 15\n",
      "Loss: 0.002875593490898609, Batch:1321/1875, Epoch: 15\n",
      "Loss: 0.0030770222656428814, Batch:1331/1875, Epoch: 15\n",
      "Loss: 0.002510965336114168, Batch:1341/1875, Epoch: 15\n",
      "Loss: 0.0027364972047507763, Batch:1351/1875, Epoch: 15\n",
      "Loss: 0.002633737400174141, Batch:1361/1875, Epoch: 15\n",
      "Loss: 0.003300683107227087, Batch:1371/1875, Epoch: 15\n",
      "Loss: 0.0030777673237025738, Batch:1381/1875, Epoch: 15\n",
      "Loss: 0.002321231411769986, Batch:1391/1875, Epoch: 15\n",
      "Loss: 0.0027944515459239483, Batch:1401/1875, Epoch: 15\n",
      "Loss: 0.0029152624774724245, Batch:1411/1875, Epoch: 15\n",
      "Loss: 0.0028125736862421036, Batch:1421/1875, Epoch: 15\n",
      "Loss: 0.003640975570306182, Batch:1431/1875, Epoch: 15\n",
      "Loss: 0.003035458270460367, Batch:1441/1875, Epoch: 15\n",
      "Loss: 0.0031707666348665953, Batch:1451/1875, Epoch: 15\n",
      "Loss: 0.0025587622076272964, Batch:1461/1875, Epoch: 15\n",
      "Loss: 0.0025028253439813852, Batch:1471/1875, Epoch: 15\n",
      "Loss: 0.0032592639327049255, Batch:1481/1875, Epoch: 15\n",
      "Loss: 0.003306860104203224, Batch:1491/1875, Epoch: 15\n",
      "Loss: 0.0033384691923856735, Batch:1501/1875, Epoch: 15\n",
      "Loss: 0.0029895207844674587, Batch:1511/1875, Epoch: 15\n",
      "Loss: 0.0022833985276520252, Batch:1521/1875, Epoch: 15\n",
      "Loss: 0.003215774428099394, Batch:1531/1875, Epoch: 15\n",
      "Loss: 0.0029382549691945314, Batch:1541/1875, Epoch: 15\n",
      "Loss: 0.002673727460205555, Batch:1551/1875, Epoch: 15\n",
      "Loss: 0.002714228816330433, Batch:1561/1875, Epoch: 15\n",
      "Loss: 0.002395341405645013, Batch:1571/1875, Epoch: 15\n",
      "Loss: 0.0026463251560926437, Batch:1581/1875, Epoch: 15\n",
      "Loss: 0.0027384113054722548, Batch:1591/1875, Epoch: 15\n",
      "Loss: 0.0029007992707192898, Batch:1601/1875, Epoch: 15\n",
      "Loss: 0.002117122057825327, Batch:1611/1875, Epoch: 15\n",
      "Loss: 0.003387564793229103, Batch:1621/1875, Epoch: 15\n",
      "Loss: 0.0025781106669455767, Batch:1631/1875, Epoch: 15\n",
      "Loss: 0.002279032487422228, Batch:1641/1875, Epoch: 15\n",
      "Loss: 0.002445823745802045, Batch:1651/1875, Epoch: 15\n",
      "Loss: 0.002305364003404975, Batch:1661/1875, Epoch: 15\n",
      "Loss: 0.0025187714491039515, Batch:1671/1875, Epoch: 15\n",
      "Loss: 0.0020795136224478483, Batch:1681/1875, Epoch: 15\n",
      "Loss: 0.0031754884403198957, Batch:1691/1875, Epoch: 15\n",
      "Loss: 0.002308974042534828, Batch:1701/1875, Epoch: 15\n",
      "Loss: 0.002475401619449258, Batch:1711/1875, Epoch: 15\n",
      "Loss: 0.003036045003682375, Batch:1721/1875, Epoch: 15\n",
      "Loss: 0.0022137730848044157, Batch:1731/1875, Epoch: 15\n",
      "Loss: 0.0028594923205673695, Batch:1741/1875, Epoch: 15\n",
      "Loss: 0.0027601365000009537, Batch:1751/1875, Epoch: 15\n",
      "Loss: 0.002788044046610594, Batch:1761/1875, Epoch: 15\n",
      "Loss: 0.003086127806454897, Batch:1771/1875, Epoch: 15\n",
      "Loss: 0.002548552816733718, Batch:1781/1875, Epoch: 15\n",
      "Loss: 0.00257474766112864, Batch:1791/1875, Epoch: 15\n",
      "Loss: 0.002334744203835726, Batch:1801/1875, Epoch: 15\n",
      "Loss: 0.00264930771663785, Batch:1811/1875, Epoch: 15\n",
      "Loss: 0.0033049227204173803, Batch:1821/1875, Epoch: 15\n",
      "Loss: 0.0027718564961105585, Batch:1831/1875, Epoch: 15\n",
      "Loss: 0.0034637353383004665, Batch:1841/1875, Epoch: 15\n",
      "Loss: 0.0033554816618561745, Batch:1851/1875, Epoch: 15\n",
      "Loss: 0.005716701038181782, Batch:1861/1875, Epoch: 15\n",
      "Loss: 0.004192535299807787, Batch:1871/1875, Epoch: 15\n",
      "Loss: 0.003994844388216734, Batch:1/1875, Epoch: 20\n",
      "Loss: 0.003222381230443716, Batch:11/1875, Epoch: 20\n",
      "Loss: 0.0028764032758772373, Batch:21/1875, Epoch: 20\n",
      "Loss: 0.0022934069857001305, Batch:31/1875, Epoch: 20\n",
      "Loss: 0.0026824616361409426, Batch:41/1875, Epoch: 20\n",
      "Loss: 0.0026670030783861876, Batch:51/1875, Epoch: 20\n",
      "Loss: 0.0026164324954152107, Batch:61/1875, Epoch: 20\n",
      "Loss: 0.002326587215065956, Batch:71/1875, Epoch: 20\n",
      "Loss: 0.0020581914577633142, Batch:81/1875, Epoch: 20\n",
      "Loss: 0.0029800741467624903, Batch:91/1875, Epoch: 20\n",
      "Loss: 0.0020121214911341667, Batch:101/1875, Epoch: 20\n",
      "Loss: 0.0020911942701786757, Batch:111/1875, Epoch: 20\n",
      "Loss: 0.0022457444574683905, Batch:121/1875, Epoch: 20\n",
      "Loss: 0.002002327237278223, Batch:131/1875, Epoch: 20\n",
      "Loss: 0.002670795191079378, Batch:141/1875, Epoch: 20\n",
      "Loss: 0.002657497301697731, Batch:151/1875, Epoch: 20\n",
      "Loss: 0.0028813201934099197, Batch:161/1875, Epoch: 20\n",
      "Loss: 0.0022905042860656977, Batch:171/1875, Epoch: 20\n",
      "Loss: 0.002427192870527506, Batch:181/1875, Epoch: 20\n",
      "Loss: 0.00265188654884696, Batch:191/1875, Epoch: 20\n",
      "Loss: 0.0020596482791006565, Batch:201/1875, Epoch: 20\n",
      "Loss: 0.0029279510490596294, Batch:211/1875, Epoch: 20\n",
      "Loss: 0.002498091896995902, Batch:221/1875, Epoch: 20\n",
      "Loss: 0.002719653071835637, Batch:231/1875, Epoch: 20\n",
      "Loss: 0.0022071998100727797, Batch:241/1875, Epoch: 20\n",
      "Loss: 0.0028979808557778597, Batch:251/1875, Epoch: 20\n",
      "Loss: 0.002328891772776842, Batch:261/1875, Epoch: 20\n",
      "Loss: 0.0023748022504150867, Batch:271/1875, Epoch: 20\n",
      "Loss: 0.001953445142135024, Batch:281/1875, Epoch: 20\n",
      "Loss: 0.0032574704382568598, Batch:291/1875, Epoch: 20\n",
      "Loss: 0.0024875709787011147, Batch:301/1875, Epoch: 20\n",
      "Loss: 0.002458160975947976, Batch:311/1875, Epoch: 20\n",
      "Loss: 0.0028555465396493673, Batch:321/1875, Epoch: 20\n",
      "Loss: 0.002432949608191848, Batch:331/1875, Epoch: 20\n",
      "Loss: 0.0020261388272047043, Batch:341/1875, Epoch: 20\n",
      "Loss: 0.0024183345958590508, Batch:351/1875, Epoch: 20\n",
      "Loss: 0.0021899896673858166, Batch:361/1875, Epoch: 20\n",
      "Loss: 0.002394898794591427, Batch:371/1875, Epoch: 20\n",
      "Loss: 0.0022474222350865602, Batch:381/1875, Epoch: 20\n",
      "Loss: 0.0021982286125421524, Batch:391/1875, Epoch: 20\n",
      "Loss: 0.002246058778837323, Batch:401/1875, Epoch: 20\n",
      "Loss: 0.002953222254291177, Batch:411/1875, Epoch: 20\n",
      "Loss: 0.002209891565144062, Batch:421/1875, Epoch: 20\n",
      "Loss: 0.0029557249508798122, Batch:431/1875, Epoch: 20\n",
      "Loss: 0.0025486268568784, Batch:441/1875, Epoch: 20\n",
      "Loss: 0.002463818062096834, Batch:451/1875, Epoch: 20\n",
      "Loss: 0.0024391377810388803, Batch:461/1875, Epoch: 20\n",
      "Loss: 0.0023932422045618296, Batch:471/1875, Epoch: 20\n",
      "Loss: 0.0024217325262725353, Batch:481/1875, Epoch: 20\n",
      "Loss: 0.00287567893974483, Batch:491/1875, Epoch: 20\n",
      "Loss: 0.0022733614314347506, Batch:501/1875, Epoch: 20\n",
      "Loss: 0.002372981980443001, Batch:511/1875, Epoch: 20\n",
      "Loss: 0.0021173139102756977, Batch:521/1875, Epoch: 20\n",
      "Loss: 0.0022926642559468746, Batch:531/1875, Epoch: 20\n",
      "Loss: 0.0023660375736653805, Batch:541/1875, Epoch: 20\n",
      "Loss: 0.002818652428686619, Batch:551/1875, Epoch: 20\n",
      "Loss: 0.0022531936410814524, Batch:561/1875, Epoch: 20\n",
      "Loss: 0.00227932445704937, Batch:571/1875, Epoch: 20\n",
      "Loss: 0.0028972455766052008, Batch:581/1875, Epoch: 20\n",
      "Loss: 0.002263482892885804, Batch:591/1875, Epoch: 20\n",
      "Loss: 0.003108029020950198, Batch:601/1875, Epoch: 20\n",
      "Loss: 0.0025117523036897182, Batch:611/1875, Epoch: 20\n",
      "Loss: 0.00233963574282825, Batch:621/1875, Epoch: 20\n",
      "Loss: 0.002613069023936987, Batch:631/1875, Epoch: 20\n",
      "Loss: 0.0019700266420841217, Batch:641/1875, Epoch: 20\n",
      "Loss: 0.0021838799584656954, Batch:651/1875, Epoch: 20\n",
      "Loss: 0.002513864077627659, Batch:661/1875, Epoch: 20\n",
      "Loss: 0.0020691428799182177, Batch:671/1875, Epoch: 20\n",
      "Loss: 0.003907516598701477, Batch:681/1875, Epoch: 20\n",
      "Loss: 0.002733144210651517, Batch:691/1875, Epoch: 20\n",
      "Loss: 0.002221411559730768, Batch:701/1875, Epoch: 20\n",
      "Loss: 0.0028647473081946373, Batch:711/1875, Epoch: 20\n",
      "Loss: 0.00343297328799963, Batch:721/1875, Epoch: 20\n",
      "Loss: 0.002658720361068845, Batch:731/1875, Epoch: 20\n",
      "Loss: 0.0024399959947913885, Batch:741/1875, Epoch: 20\n",
      "Loss: 0.002546320203691721, Batch:751/1875, Epoch: 20\n",
      "Loss: 0.0019470060942694545, Batch:761/1875, Epoch: 20\n",
      "Loss: 0.0023386559914797544, Batch:771/1875, Epoch: 20\n",
      "Loss: 0.001996932551264763, Batch:781/1875, Epoch: 20\n",
      "Loss: 0.0034424783661961555, Batch:791/1875, Epoch: 20\n",
      "Loss: 0.0023216695990413427, Batch:801/1875, Epoch: 20\n",
      "Loss: 0.0023076008073985577, Batch:811/1875, Epoch: 20\n",
      "Loss: 0.002396034076809883, Batch:821/1875, Epoch: 20\n",
      "Loss: 0.002613428747281432, Batch:831/1875, Epoch: 20\n",
      "Loss: 0.0023546412121504545, Batch:841/1875, Epoch: 20\n",
      "Loss: 0.002823305083438754, Batch:851/1875, Epoch: 20\n",
      "Loss: 0.002219143556430936, Batch:861/1875, Epoch: 20\n",
      "Loss: 0.0024611165281385183, Batch:871/1875, Epoch: 20\n",
      "Loss: 0.002497308421880007, Batch:881/1875, Epoch: 20\n",
      "Loss: 0.002390498761087656, Batch:891/1875, Epoch: 20\n",
      "Loss: 0.002153563080355525, Batch:901/1875, Epoch: 20\n",
      "Loss: 0.0031369109638035297, Batch:911/1875, Epoch: 20\n",
      "Loss: 0.0022216562647372484, Batch:921/1875, Epoch: 20\n",
      "Loss: 0.002514516469091177, Batch:931/1875, Epoch: 20\n",
      "Loss: 0.002751233521848917, Batch:941/1875, Epoch: 20\n",
      "Loss: 0.0024780812673270702, Batch:951/1875, Epoch: 20\n",
      "Loss: 0.002646898152306676, Batch:961/1875, Epoch: 20\n",
      "Loss: 0.0029807223472744226, Batch:971/1875, Epoch: 20\n",
      "Loss: 0.0022970964200794697, Batch:981/1875, Epoch: 20\n",
      "Loss: 0.0029660495929419994, Batch:991/1875, Epoch: 20\n",
      "Loss: 0.002812158316373825, Batch:1001/1875, Epoch: 20\n",
      "Loss: 0.0027583609335124493, Batch:1011/1875, Epoch: 20\n",
      "Loss: 0.002387598156929016, Batch:1021/1875, Epoch: 20\n",
      "Loss: 0.002579418243840337, Batch:1031/1875, Epoch: 20\n",
      "Loss: 0.0026205217000097036, Batch:1041/1875, Epoch: 20\n",
      "Loss: 0.0025870325043797493, Batch:1051/1875, Epoch: 20\n",
      "Loss: 0.002711094683036208, Batch:1061/1875, Epoch: 20\n",
      "Loss: 0.002159728901460767, Batch:1071/1875, Epoch: 20\n",
      "Loss: 0.002447412582114339, Batch:1081/1875, Epoch: 20\n",
      "Loss: 0.002362126251682639, Batch:1091/1875, Epoch: 20\n",
      "Loss: 0.0024843928404152393, Batch:1101/1875, Epoch: 20\n",
      "Loss: 0.0028693457134068012, Batch:1111/1875, Epoch: 20\n",
      "Loss: 0.0023803585208952427, Batch:1121/1875, Epoch: 20\n",
      "Loss: 0.0036894921213388443, Batch:1131/1875, Epoch: 20\n",
      "Loss: 0.0033895347733050585, Batch:1141/1875, Epoch: 20\n",
      "Loss: 0.003541664220392704, Batch:1151/1875, Epoch: 20\n",
      "Loss: 0.002945014974102378, Batch:1161/1875, Epoch: 20\n",
      "Loss: 0.0031723727006465197, Batch:1171/1875, Epoch: 20\n",
      "Loss: 0.0033653161954134703, Batch:1181/1875, Epoch: 20\n",
      "Loss: 0.002855018712580204, Batch:1191/1875, Epoch: 20\n",
      "Loss: 0.0025325824972242117, Batch:1201/1875, Epoch: 20\n",
      "Loss: 0.0025774871464818716, Batch:1211/1875, Epoch: 20\n",
      "Loss: 0.0021548920776695013, Batch:1221/1875, Epoch: 20\n",
      "Loss: 0.0022506238892674446, Batch:1231/1875, Epoch: 20\n",
      "Loss: 0.002214385662227869, Batch:1241/1875, Epoch: 20\n",
      "Loss: 0.0027272929437458515, Batch:1251/1875, Epoch: 20\n",
      "Loss: 0.0021600783802568913, Batch:1261/1875, Epoch: 20\n",
      "Loss: 0.0027469992637634277, Batch:1271/1875, Epoch: 20\n",
      "Loss: 0.0026186832692474127, Batch:1281/1875, Epoch: 20\n",
      "Loss: 0.003119403962045908, Batch:1291/1875, Epoch: 20\n",
      "Loss: 0.0025154214818030596, Batch:1301/1875, Epoch: 20\n",
      "Loss: 0.0024463713634759188, Batch:1311/1875, Epoch: 20\n",
      "Loss: 0.002486682962626219, Batch:1321/1875, Epoch: 20\n",
      "Loss: 0.0027027272153645754, Batch:1331/1875, Epoch: 20\n",
      "Loss: 0.0022459605243057013, Batch:1341/1875, Epoch: 20\n",
      "Loss: 0.002427418250590563, Batch:1351/1875, Epoch: 20\n",
      "Loss: 0.002413159003481269, Batch:1361/1875, Epoch: 20\n",
      "Loss: 0.0029684307519346476, Batch:1371/1875, Epoch: 20\n",
      "Loss: 0.0027689491398632526, Batch:1381/1875, Epoch: 20\n",
      "Loss: 0.002132162917405367, Batch:1391/1875, Epoch: 20\n",
      "Loss: 0.002338818274438381, Batch:1401/1875, Epoch: 20\n",
      "Loss: 0.0026492697652429342, Batch:1411/1875, Epoch: 20\n",
      "Loss: 0.0025523218791931868, Batch:1421/1875, Epoch: 20\n",
      "Loss: 0.0034655441995710135, Batch:1431/1875, Epoch: 20\n",
      "Loss: 0.0027163003105670214, Batch:1441/1875, Epoch: 20\n",
      "Loss: 0.0027621560730040073, Batch:1451/1875, Epoch: 20\n",
      "Loss: 0.002190076047554612, Batch:1461/1875, Epoch: 20\n",
      "Loss: 0.002166557125747204, Batch:1471/1875, Epoch: 20\n",
      "Loss: 0.0029427846893668175, Batch:1481/1875, Epoch: 20\n",
      "Loss: 0.002812202787026763, Batch:1491/1875, Epoch: 20\n",
      "Loss: 0.002920563565567136, Batch:1501/1875, Epoch: 20\n",
      "Loss: 0.0027872095815837383, Batch:1511/1875, Epoch: 20\n",
      "Loss: 0.0021802245173603296, Batch:1521/1875, Epoch: 20\n",
      "Loss: 0.0028964532539248466, Batch:1531/1875, Epoch: 20\n",
      "Loss: 0.0024929598439484835, Batch:1541/1875, Epoch: 20\n",
      "Loss: 0.002407994121313095, Batch:1551/1875, Epoch: 20\n",
      "Loss: 0.0024670534767210484, Batch:1561/1875, Epoch: 20\n",
      "Loss: 0.002180100418627262, Batch:1571/1875, Epoch: 20\n",
      "Loss: 0.002404375933110714, Batch:1581/1875, Epoch: 20\n",
      "Loss: 0.0025653759948909283, Batch:1591/1875, Epoch: 20\n",
      "Loss: 0.002640255494043231, Batch:1601/1875, Epoch: 20\n",
      "Loss: 0.0019877587910741568, Batch:1611/1875, Epoch: 20\n",
      "Loss: 0.0029426903929561377, Batch:1621/1875, Epoch: 20\n",
      "Loss: 0.0023196509573608637, Batch:1631/1875, Epoch: 20\n",
      "Loss: 0.002076691947877407, Batch:1641/1875, Epoch: 20\n",
      "Loss: 0.00219713500700891, Batch:1651/1875, Epoch: 20\n",
      "Loss: 0.002035852987319231, Batch:1661/1875, Epoch: 20\n",
      "Loss: 0.0022872278932482004, Batch:1671/1875, Epoch: 20\n",
      "Loss: 0.0018458511913195252, Batch:1681/1875, Epoch: 20\n",
      "Loss: 0.0027299541980028152, Batch:1691/1875, Epoch: 20\n",
      "Loss: 0.002121811965480447, Batch:1701/1875, Epoch: 20\n",
      "Loss: 0.0021502883173525333, Batch:1711/1875, Epoch: 20\n",
      "Loss: 0.002817082218825817, Batch:1721/1875, Epoch: 20\n",
      "Loss: 0.001941489172168076, Batch:1731/1875, Epoch: 20\n",
      "Loss: 0.0027089568320661783, Batch:1741/1875, Epoch: 20\n",
      "Loss: 0.0024822114501148462, Batch:1751/1875, Epoch: 20\n",
      "Loss: 0.002456378424540162, Batch:1761/1875, Epoch: 20\n",
      "Loss: 0.002887121168896556, Batch:1771/1875, Epoch: 20\n",
      "Loss: 0.002286614617332816, Batch:1781/1875, Epoch: 20\n",
      "Loss: 0.0022805731277912855, Batch:1791/1875, Epoch: 20\n",
      "Loss: 0.0021275379694998264, Batch:1801/1875, Epoch: 20\n",
      "Loss: 0.002368520712479949, Batch:1811/1875, Epoch: 20\n",
      "Loss: 0.0029934796039015055, Batch:1821/1875, Epoch: 20\n",
      "Loss: 0.0023810609709471464, Batch:1831/1875, Epoch: 20\n",
      "Loss: 0.002946944208815694, Batch:1841/1875, Epoch: 20\n",
      "Loss: 0.0028503024950623512, Batch:1851/1875, Epoch: 20\n",
      "Loss: 0.00463452422991395, Batch:1861/1875, Epoch: 20\n",
      "Loss: 0.0037501100450754166, Batch:1871/1875, Epoch: 20\n",
      "Loss: 0.0034849257208406925, Batch:1/1875, Epoch: 25\n",
      "Loss: 0.0028544876258820295, Batch:11/1875, Epoch: 25\n",
      "Loss: 0.0025488913524895906, Batch:21/1875, Epoch: 25\n",
      "Loss: 0.0020281742326915264, Batch:31/1875, Epoch: 25\n",
      "Loss: 0.0024484647437930107, Batch:41/1875, Epoch: 25\n",
      "Loss: 0.002505748299881816, Batch:51/1875, Epoch: 25\n",
      "Loss: 0.0023956482764333487, Batch:61/1875, Epoch: 25\n",
      "Loss: 0.002063184976577759, Batch:71/1875, Epoch: 25\n",
      "Loss: 0.0018902079900726676, Batch:81/1875, Epoch: 25\n",
      "Loss: 0.002765878103673458, Batch:91/1875, Epoch: 25\n",
      "Loss: 0.0018739973893389106, Batch:101/1875, Epoch: 25\n",
      "Loss: 0.0018370007164776325, Batch:111/1875, Epoch: 25\n",
      "Loss: 0.0020510090980678797, Batch:121/1875, Epoch: 25\n",
      "Loss: 0.001817842130549252, Batch:131/1875, Epoch: 25\n",
      "Loss: 0.002423603553324938, Batch:141/1875, Epoch: 25\n",
      "Loss: 0.0024033321533352137, Batch:151/1875, Epoch: 25\n",
      "Loss: 0.0024749967269599438, Batch:161/1875, Epoch: 25\n",
      "Loss: 0.0020431960001587868, Batch:171/1875, Epoch: 25\n",
      "Loss: 0.0021864112932235003, Batch:181/1875, Epoch: 25\n",
      "Loss: 0.0024960231967270374, Batch:191/1875, Epoch: 25\n",
      "Loss: 0.0019473256543278694, Batch:201/1875, Epoch: 25\n",
      "Loss: 0.0026083686389029026, Batch:211/1875, Epoch: 25\n",
      "Loss: 0.002105332911014557, Batch:221/1875, Epoch: 25\n",
      "Loss: 0.002489356556907296, Batch:231/1875, Epoch: 25\n",
      "Loss: 0.0020675053820014, Batch:241/1875, Epoch: 25\n",
      "Loss: 0.0027119724545627832, Batch:251/1875, Epoch: 25\n",
      "Loss: 0.0021257202606648207, Batch:261/1875, Epoch: 25\n",
      "Loss: 0.0022906982339918613, Batch:271/1875, Epoch: 25\n",
      "Loss: 0.0017777394969016314, Batch:281/1875, Epoch: 25\n",
      "Loss: 0.002962869592010975, Batch:291/1875, Epoch: 25\n",
      "Loss: 0.002325292443856597, Batch:301/1875, Epoch: 25\n",
      "Loss: 0.0022253054194152355, Batch:311/1875, Epoch: 25\n",
      "Loss: 0.0024453222285956144, Batch:321/1875, Epoch: 25\n",
      "Loss: 0.002170683816075325, Batch:331/1875, Epoch: 25\n",
      "Loss: 0.0019879601895809174, Batch:341/1875, Epoch: 25\n",
      "Loss: 0.002278416883200407, Batch:351/1875, Epoch: 25\n",
      "Loss: 0.0019613851327449083, Batch:361/1875, Epoch: 25\n",
      "Loss: 0.0021590653341263533, Batch:371/1875, Epoch: 25\n",
      "Loss: 0.002102265600115061, Batch:381/1875, Epoch: 25\n",
      "Loss: 0.002096411306411028, Batch:391/1875, Epoch: 25\n",
      "Loss: 0.0020819345954805613, Batch:401/1875, Epoch: 25\n",
      "Loss: 0.002552919089794159, Batch:411/1875, Epoch: 25\n",
      "Loss: 0.00209545879624784, Batch:421/1875, Epoch: 25\n",
      "Loss: 0.0025095033925026655, Batch:431/1875, Epoch: 25\n",
      "Loss: 0.0023320310283452272, Batch:441/1875, Epoch: 25\n",
      "Loss: 0.0023720033932477236, Batch:451/1875, Epoch: 25\n",
      "Loss: 0.0022974631283432245, Batch:461/1875, Epoch: 25\n",
      "Loss: 0.0022394349798560143, Batch:471/1875, Epoch: 25\n",
      "Loss: 0.0023499871604144573, Batch:481/1875, Epoch: 25\n",
      "Loss: 0.002773294923827052, Batch:491/1875, Epoch: 25\n",
      "Loss: 0.0021102239843457937, Batch:501/1875, Epoch: 25\n",
      "Loss: 0.0021686030086129904, Batch:511/1875, Epoch: 25\n",
      "Loss: 0.002041888190433383, Batch:521/1875, Epoch: 25\n",
      "Loss: 0.0021135874558240175, Batch:531/1875, Epoch: 25\n",
      "Loss: 0.002099772682413459, Batch:541/1875, Epoch: 25\n",
      "Loss: 0.002590552205219865, Batch:551/1875, Epoch: 25\n",
      "Loss: 0.0020878100767731667, Batch:561/1875, Epoch: 25\n",
      "Loss: 0.002052152529358864, Batch:571/1875, Epoch: 25\n",
      "Loss: 0.0026998003013432026, Batch:581/1875, Epoch: 25\n",
      "Loss: 0.001994351390749216, Batch:591/1875, Epoch: 25\n",
      "Loss: 0.0027522663585841656, Batch:601/1875, Epoch: 25\n",
      "Loss: 0.002259195316582918, Batch:611/1875, Epoch: 25\n",
      "Loss: 0.002167284023016691, Batch:621/1875, Epoch: 25\n",
      "Loss: 0.0024164924398064613, Batch:631/1875, Epoch: 25\n",
      "Loss: 0.0018043974414467812, Batch:641/1875, Epoch: 25\n",
      "Loss: 0.002063568215817213, Batch:651/1875, Epoch: 25\n",
      "Loss: 0.0023798076435923576, Batch:661/1875, Epoch: 25\n",
      "Loss: 0.0018089406657963991, Batch:671/1875, Epoch: 25\n",
      "Loss: 0.0031757415272295475, Batch:681/1875, Epoch: 25\n",
      "Loss: 0.002491728402674198, Batch:691/1875, Epoch: 25\n",
      "Loss: 0.0019400549354031682, Batch:701/1875, Epoch: 25\n",
      "Loss: 0.002675372641533613, Batch:711/1875, Epoch: 25\n",
      "Loss: 0.0030766527634114027, Batch:721/1875, Epoch: 25\n",
      "Loss: 0.002476964145898819, Batch:731/1875, Epoch: 25\n",
      "Loss: 0.002247344935312867, Batch:741/1875, Epoch: 25\n",
      "Loss: 0.0023684322368353605, Batch:751/1875, Epoch: 25\n",
      "Loss: 0.0018036221154034138, Batch:761/1875, Epoch: 25\n",
      "Loss: 0.0021218026522547007, Batch:771/1875, Epoch: 25\n",
      "Loss: 0.0018349757883697748, Batch:781/1875, Epoch: 25\n",
      "Loss: 0.0029841556679457426, Batch:791/1875, Epoch: 25\n",
      "Loss: 0.0021464526653289795, Batch:801/1875, Epoch: 25\n",
      "Loss: 0.002121984027326107, Batch:811/1875, Epoch: 25\n",
      "Loss: 0.0021724828984588385, Batch:821/1875, Epoch: 25\n",
      "Loss: 0.002253339858725667, Batch:831/1875, Epoch: 25\n",
      "Loss: 0.002117660827934742, Batch:841/1875, Epoch: 25\n",
      "Loss: 0.0026483864057809114, Batch:851/1875, Epoch: 25\n",
      "Loss: 0.001976801548153162, Batch:861/1875, Epoch: 25\n",
      "Loss: 0.0023082303814589977, Batch:871/1875, Epoch: 25\n",
      "Loss: 0.0023137042298913, Batch:881/1875, Epoch: 25\n",
      "Loss: 0.0023068662267178297, Batch:891/1875, Epoch: 25\n",
      "Loss: 0.002084864303469658, Batch:901/1875, Epoch: 25\n",
      "Loss: 0.0028835954144597054, Batch:911/1875, Epoch: 25\n",
      "Loss: 0.0020523660350590944, Batch:921/1875, Epoch: 25\n",
      "Loss: 0.0023552842903882265, Batch:931/1875, Epoch: 25\n",
      "Loss: 0.002547874581068754, Batch:941/1875, Epoch: 25\n",
      "Loss: 0.0021838441025465727, Batch:951/1875, Epoch: 25\n",
      "Loss: 0.0024888277985155582, Batch:961/1875, Epoch: 25\n",
      "Loss: 0.0027405519504100084, Batch:971/1875, Epoch: 25\n",
      "Loss: 0.0020879609510302544, Batch:981/1875, Epoch: 25\n",
      "Loss: 0.0027504791505634785, Batch:991/1875, Epoch: 25\n",
      "Loss: 0.002625252353027463, Batch:1001/1875, Epoch: 25\n",
      "Loss: 0.0024744730908423662, Batch:1011/1875, Epoch: 25\n",
      "Loss: 0.0022226509172469378, Batch:1021/1875, Epoch: 25\n",
      "Loss: 0.0023446762934327126, Batch:1031/1875, Epoch: 25\n",
      "Loss: 0.00247994065284729, Batch:1041/1875, Epoch: 25\n",
      "Loss: 0.0024547800421714783, Batch:1051/1875, Epoch: 25\n",
      "Loss: 0.0023948578163981438, Batch:1061/1875, Epoch: 25\n",
      "Loss: 0.002055948367342353, Batch:1071/1875, Epoch: 25\n",
      "Loss: 0.0023102310951799154, Batch:1081/1875, Epoch: 25\n",
      "Loss: 0.0021644970402121544, Batch:1091/1875, Epoch: 25\n",
      "Loss: 0.0022906367667019367, Batch:1101/1875, Epoch: 25\n",
      "Loss: 0.0026891534216701984, Batch:1111/1875, Epoch: 25\n",
      "Loss: 0.002212031977251172, Batch:1121/1875, Epoch: 25\n",
      "Loss: 0.0034038450103253126, Batch:1131/1875, Epoch: 25\n",
      "Loss: 0.00298460409976542, Batch:1141/1875, Epoch: 25\n",
      "Loss: 0.0031350303906947374, Batch:1151/1875, Epoch: 25\n",
      "Loss: 0.0026844891253858805, Batch:1161/1875, Epoch: 25\n",
      "Loss: 0.002848847769200802, Batch:1171/1875, Epoch: 25\n",
      "Loss: 0.003048329846933484, Batch:1181/1875, Epoch: 25\n",
      "Loss: 0.002711595967411995, Batch:1191/1875, Epoch: 25\n",
      "Loss: 0.002424701349809766, Batch:1201/1875, Epoch: 25\n",
      "Loss: 0.002326642395928502, Batch:1211/1875, Epoch: 25\n",
      "Loss: 0.002098430646583438, Batch:1221/1875, Epoch: 25\n",
      "Loss: 0.00206205272115767, Batch:1231/1875, Epoch: 25\n",
      "Loss: 0.002192068612203002, Batch:1241/1875, Epoch: 25\n",
      "Loss: 0.002484857337549329, Batch:1251/1875, Epoch: 25\n",
      "Loss: 0.001992523204535246, Batch:1261/1875, Epoch: 25\n",
      "Loss: 0.0025424552150070667, Batch:1271/1875, Epoch: 25\n",
      "Loss: 0.0024083915632218122, Batch:1281/1875, Epoch: 25\n",
      "Loss: 0.002740868367254734, Batch:1291/1875, Epoch: 25\n",
      "Loss: 0.0024272215086966753, Batch:1301/1875, Epoch: 25\n",
      "Loss: 0.002223535208031535, Batch:1311/1875, Epoch: 25\n",
      "Loss: 0.002319721505045891, Batch:1321/1875, Epoch: 25\n",
      "Loss: 0.0024325507692992687, Batch:1331/1875, Epoch: 25\n",
      "Loss: 0.001986186485737562, Batch:1341/1875, Epoch: 25\n",
      "Loss: 0.0021986407227814198, Batch:1351/1875, Epoch: 25\n",
      "Loss: 0.002220075111836195, Batch:1361/1875, Epoch: 25\n",
      "Loss: 0.002710332628339529, Batch:1371/1875, Epoch: 25\n",
      "Loss: 0.0024845681618899107, Batch:1381/1875, Epoch: 25\n",
      "Loss: 0.002063177991658449, Batch:1391/1875, Epoch: 25\n",
      "Loss: 0.0021672695875167847, Batch:1401/1875, Epoch: 25\n",
      "Loss: 0.0024753145407885313, Batch:1411/1875, Epoch: 25\n",
      "Loss: 0.0023599250707775354, Batch:1421/1875, Epoch: 25\n",
      "Loss: 0.0031682532280683517, Batch:1431/1875, Epoch: 25\n",
      "Loss: 0.002391562331467867, Batch:1441/1875, Epoch: 25\n",
      "Loss: 0.0024744290858507156, Batch:1451/1875, Epoch: 25\n",
      "Loss: 0.0020335724111646414, Batch:1461/1875, Epoch: 25\n",
      "Loss: 0.001923248404636979, Batch:1471/1875, Epoch: 25\n",
      "Loss: 0.0027674385346472263, Batch:1481/1875, Epoch: 25\n",
      "Loss: 0.002500624395906925, Batch:1491/1875, Epoch: 25\n",
      "Loss: 0.002688465639948845, Batch:1501/1875, Epoch: 25\n",
      "Loss: 0.0024459590204060078, Batch:1511/1875, Epoch: 25\n",
      "Loss: 0.0019779468420892954, Batch:1521/1875, Epoch: 25\n",
      "Loss: 0.0025458165910094976, Batch:1531/1875, Epoch: 25\n",
      "Loss: 0.0022788969799876213, Batch:1541/1875, Epoch: 25\n",
      "Loss: 0.002160948934033513, Batch:1551/1875, Epoch: 25\n",
      "Loss: 0.002273373305797577, Batch:1561/1875, Epoch: 25\n",
      "Loss: 0.0018937478307634592, Batch:1571/1875, Epoch: 25\n",
      "Loss: 0.002164970152080059, Batch:1581/1875, Epoch: 25\n",
      "Loss: 0.0022982072550803423, Batch:1591/1875, Epoch: 25\n",
      "Loss: 0.0023587895557284355, Batch:1601/1875, Epoch: 25\n",
      "Loss: 0.001811221824027598, Batch:1611/1875, Epoch: 25\n",
      "Loss: 0.002663921331986785, Batch:1621/1875, Epoch: 25\n",
      "Loss: 0.002148618921637535, Batch:1631/1875, Epoch: 25\n",
      "Loss: 0.0017529120668768883, Batch:1641/1875, Epoch: 25\n",
      "Loss: 0.0020251807291060686, Batch:1651/1875, Epoch: 25\n",
      "Loss: 0.001915732747875154, Batch:1661/1875, Epoch: 25\n",
      "Loss: 0.0020797152537852526, Batch:1671/1875, Epoch: 25\n",
      "Loss: 0.0016908615361899137, Batch:1681/1875, Epoch: 25\n",
      "Loss: 0.0025585030671209097, Batch:1691/1875, Epoch: 25\n",
      "Loss: 0.0018604248762130737, Batch:1701/1875, Epoch: 25\n",
      "Loss: 0.0020898485090583563, Batch:1711/1875, Epoch: 25\n",
      "Loss: 0.0027209403924643993, Batch:1721/1875, Epoch: 25\n",
      "Loss: 0.001803081831894815, Batch:1731/1875, Epoch: 25\n",
      "Loss: 0.0024697822518646717, Batch:1741/1875, Epoch: 25\n",
      "Loss: 0.0021987382788211107, Batch:1751/1875, Epoch: 25\n",
      "Loss: 0.0022601806558668613, Batch:1761/1875, Epoch: 25\n",
      "Loss: 0.002599910134449601, Batch:1771/1875, Epoch: 25\n",
      "Loss: 0.0021841851994395256, Batch:1781/1875, Epoch: 25\n",
      "Loss: 0.0021357559598982334, Batch:1791/1875, Epoch: 25\n",
      "Loss: 0.0019835878629237413, Batch:1801/1875, Epoch: 25\n",
      "Loss: 0.0021354288328438997, Batch:1811/1875, Epoch: 25\n",
      "Loss: 0.002982751466333866, Batch:1821/1875, Epoch: 25\n",
      "Loss: 0.0020801357459276915, Batch:1831/1875, Epoch: 25\n",
      "Loss: 0.0028711676131933928, Batch:1841/1875, Epoch: 25\n",
      "Loss: 0.0028718197718262672, Batch:1851/1875, Epoch: 25\n",
      "Loss: 0.00400055106729269, Batch:1861/1875, Epoch: 25\n",
      "Loss: 0.0034960361663252115, Batch:1871/1875, Epoch: 25\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 1e-3)\n",
    "train_dataloader = DataLoader(train_data, batch_size = batch_size)\n",
    "\n",
    "model.train()\n",
    "for epoch in tqdm(range(n_epochs)):\n",
    "    for batch_idx, (X,y) in enumerate(train_dataloader):\n",
    "        X = X.to(device)\n",
    "        X_ciag = add_symbol(X.clone()) ## !!!!\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        prediction = model(X_ciag)\n",
    "        loss = criterion(prediction, X) \n",
    "        \n",
    "        if not (torch.isnan(loss) | torch.isinf(loss)):\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (batch_idx + 250) % 10 == 0 and (epoch + 1) % 5 == 0:\n",
    "            text = f'Loss: {loss}, Batch:{batch_idx + 1}/{len(train_dataloader)}, Epoch: {epoch + 1}'\n",
    "            print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the model on the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = DataLoader(test_data, batch_size = 256, shuffle = True, drop_last = True)\n",
    "\n",
    "original = np.zeros(shape = (len(test_dataloader), 256, 1, 28, 28))\n",
    "predictions = np.zeros(shape = (len(test_dataloader), 256, 1, 28, 28))\n",
    "labels = np.zeros(shape = (len(test_dataloader), 256))\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (X,y) in enumerate(test_dataloader):\n",
    "        X = X.to(device)\n",
    "        X_ciag = add_symbol(X.clone()) ####\n",
    "        pred = model(X_ciag)\n",
    "        pred = pred.cpu().numpy()\n",
    "        predictions[batch_idx] = pred\n",
    "        original[batch_idx] = X_ciag.cpu().numpy()\n",
    "        labels[batch_idx] = y.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 20 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "figure = plt.figure(figsize = (20,4))\n",
    "cols, rows = 10, 2\n",
    "\n",
    "for i in range (1, cols + 1):\n",
    "    \n",
    "    sample_idx = torch.randint(len(test_dataloader), size = (1,)).item()\n",
    "    batch_idx = torch.randint(batch_size, size = (1,)).item()\n",
    "    generated_image = predictions[sample_idx, batch_idx, 0]\n",
    "    original_image = original[sample_idx, batch_idx, 0]\n",
    "    \n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(original_image, cmap = 'gray')\n",
    "    plt.title('Ground Truth')\n",
    "    \n",
    "    figure.add_subplot(rows, cols, i + cols)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(generated_image, cmap='gray')\n",
    "    plt.title('Reconstructed')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
