{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac02c96-875c-4076-9e1c-c45393de37fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import datetime\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501f27df-8221-40c5-a5bf-3279483e53ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'epochs': 200,\n",
    "    'img_size': 256,\n",
    "    'batch_size': 32,\n",
    "    'cuda': True if torch.cuda.is_available() else False,\n",
    "    'sample_interval': 20,\n",
    "    'checkpoint_interval': -1,\n",
    "    'dataset_name': 'facade',\n",
    "\n",
    "    'lr': 0.0002,\n",
    "    'b1': 0.5,\n",
    "    'b2': 0.999,\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args['device'] = torch.device('cuda')\n",
    "else:\n",
    "    args['device'] = torch.device('cpu')\n",
    "\n",
    "print(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9f52cc-cf1a-411d-bedb-c9f53417eb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ## download dos dados Facades\n",
    "# !mkdir 'facades'\n",
    "\n",
    "# !wget -N https://cmp.felk.cvut.cz/~tylecr1/facade/CMP_facade_DB_base.zip\n",
    "# !unzip -o CMP_facade_DB_base.zip -d ./facades/\n",
    "\n",
    "# !wget -N https://cmp.felk.cvut.cz/~tylecr1/facade/CMP_facade_DB_extended.zip\n",
    "# !unzip -o CMP_facade_DB_extended.zip -d ./facades/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be7903e-c7cd-4de0-9a08-bbd1a578be4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Função que recebe um \"mapa de classes\", como os mostrados acima, para uma imagem colorida\n",
    "# cada classe é mapeada para uma cor diferente, de acordo com o dicionário \"color_map\"\n",
    "def label2color_map(mask, color_map):\n",
    "    h, w = mask.shape\n",
    "    mask_img = np.zeros(shape=[h, w, 3])\n",
    "\n",
    "    for i in np.unique(mask):\n",
    "        mask_img[np.where(mask == i)] = color_map[i]\n",
    "\n",
    "    return (mask_img.transpose(2, 0, 1) / 127.5) - 1.0\n",
    "\n",
    "# Implementando um Dataset personalizado para ler os dados que acabamos de fazer o download\n",
    "class FacadesDataset(Dataset):\n",
    "    def __init__(self, root = '/pgeoprj/ciag2023/datasets/facades/base/', transforms_= None, max_label=12):\n",
    "        self.transform = transforms_\n",
    "        self.root = root\n",
    "        self.max_label = max_label\n",
    "\n",
    "        self.normalize = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize( mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5) )\n",
    "        ])\n",
    "\n",
    "        ## Aqui criamos um mapa de cores, que determina a cor que cada classe tem na máscara de classes\n",
    "        ## Esses valores podem ser alterados livremente, porém vale a pena sempre verificar se não existem\n",
    "        ## duas classes diferentes com a mesma cor (ou cores muito parecidas.)\n",
    "        self.color_maps = {\n",
    "            1: [0, 0, 0],\n",
    "            2: [254, 127, 45],\n",
    "            3: [252, 202, 70],\n",
    "            4: [161, 193, 129],\n",
    "            5: [97, 155, 138],\n",
    "            6: [35, 61, 77],\n",
    "            7: [120, 0, 0],\n",
    "            8: [193, 18, 31],\n",
    "            9: [253, 240, 213],\n",
    "            10: [102, 155, 188],\n",
    "            11: [254, 109, 115],\n",
    "            12: [106, 153, 78]\n",
    "        }\n",
    "\n",
    "        files_list = os.listdir(root)\n",
    "        files_list = [s.split('.')[0] for s in files_list]\n",
    "        self.files_list = np.unique(files_list)[1:]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_file = self.files_list[idx] + '.jpg'\n",
    "        mask_file = self.files_list[idx] + '.png'\n",
    "\n",
    "        img = Image.open( os.path.join(self.root, img_file) )\n",
    "        mask = Image.open( os.path.join(self.root, mask_file) )\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.normalize( self.transform(img) )\n",
    "            mask = self.transform(mask)\n",
    "\n",
    "        img = np.array(img)\n",
    "        mask = label2color_map(np.array(mask), self.color_maps)\n",
    "\n",
    "        return img.astype(np.float32), mask.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files_list)\n",
    "\n",
    "transforms_ = transforms.Compose([\n",
    "    transforms.Resize( size=(args['img_size'], args['img_size']) ),\n",
    "])\n",
    "\n",
    "facades_data = FacadesDataset(transforms_=transforms_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd70d8-f897-46a3-b147-785749b8842f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b668e76a-3954-4abb-9ea4-7329c6188a82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "facades_dataloader = DataLoader(dataset = facades_data,\n",
    "                                batch_size = args['batch_size'],\n",
    "                                shuffle = True,\n",
    "                                num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b847293-759c-4125-b64e-5a6235c596d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0603b60c-c5ff-4a9e-88da-ae40efd5b5c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
    "lambda_pixel = 100\n",
    "\n",
    "# Calculate output of image discriminator (PatchGAN)\n",
    "patch = (1, args['img_size'] // 2 ** 4, args['img_size'] // 2 ** 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8868683e-5fec-4152-b515-409c5d5c111c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models import GeneratorUNet, Discriminator\n",
    "generator = GeneratorUNet().to(args['device'])\n",
    "discriminator = Discriminator().to(args['device'])\n",
    "\n",
    "# Initialize weights\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcba222-4da5-4103-90ec-e5a7f139c2d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_images(data_loader):\n",
    "    \"\"\"Saves a generated sample from the validation set\"\"\"\n",
    "    imgs_real, mask = next( iter(data_loader) )\n",
    "    imgs_real = imgs_real.to(args['device'])\n",
    "    mask = mask.to(args['device'])\n",
    "\n",
    "    imgs_fake = generator(mask)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=min(imgs_real.size(0), 2), ncols=3, figsize=(12, 8))\n",
    "\n",
    "    for i in range( min(imgs_real.size(0), 2) ):\n",
    "\n",
    "        ax[i, 0].imshow(imgs_real.data[i].cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "        ax[i, 0].set_yticks([])\n",
    "        ax[i, 0].set_xticks([])\n",
    "        ax[i, 0].set_title('Goal')\n",
    "\n",
    "        ax[i, 1].imshow(imgs_fake.data[i].cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "        ax[i, 1].set_yticks([])\n",
    "        ax[i, 1].set_xticks([])\n",
    "        ax[i, 1].set_title('Generated')\n",
    "\n",
    "        ax[i, 2].imshow(mask.data[i].cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "        ax[i, 2].set_yticks([])\n",
    "        ax[i, 2].set_xticks([])\n",
    "        ax[i, 2].set_title('Mask')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63c5282-1b26-47b5-89ee-51bd326b9c85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_images(facades_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a5c7e1-dfc0-4ac9-92d9-a38c2ecc623d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam( generator.parameters(),\n",
    "                               lr = args['lr'],\n",
    "                               betas = (args['b1'], args['b2']) )\n",
    "\n",
    "optimizer_D = torch.optim.Adam( discriminator.parameters(),\n",
    "                               lr = args['lr'],\n",
    "                               betas = (args['b1'], args['b2']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a85a2c5a-9622-4472-9c2a-3cce5a6573ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#  Training ####################################################################\n",
    "################################################################################\n",
    "\n",
    "prev_time = time.time()\n",
    "\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "\n",
    "    for i, (real_img, mask) in enumerate(facades_dataloader):\n",
    "\n",
    "        # Transfer images and masks to GPU\n",
    "        real_img = real_img.to(args['device'])\n",
    "        mask = mask.to(args['device'])\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        y_true = torch.ones(size=(real_img.size(0), *patch), requires_grad=False).to(args['device'])\n",
    "        y_fake = torch.zeros(size=(real_img.size(0), *patch), requires_grad=False).to(args['device'])\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        # Clearing gradients for G optimizer.\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # GAN loss.\n",
    "        fake_img = generator(mask)\n",
    "        pred_fake = discriminator(fake_img, mask)\n",
    "        loss_GAN = criterion_GAN(pred_fake, y_true)\n",
    "\n",
    "        # Pixel-wise loss\n",
    "        loss_pixel = criterion_pixelwise(fake_img, real_img)\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = (loss_GAN + lambda_pixel * loss_pixel) / (1 + lambda_pixel)\n",
    "\n",
    "        # G backward and optimizer step.\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Clearing gradients for D optimizer.\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = discriminator(real_img, mask)\n",
    "        loss_real = criterion_GAN(pred_real, y_true)\n",
    "\n",
    "        # Fake loss\n",
    "        pred_fake = discriminator(fake_img.detach(), mask)\n",
    "        loss_fake = criterion_GAN(pred_fake, y_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D = 0.5 * (loss_real + loss_fake)\n",
    "\n",
    "        # D backward and optimizer step.\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(facades_dataloader) + i\n",
    "        batches_left = args['epochs'] * len(facades_dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # If at sample interval save image\n",
    "        if batches_done % args['sample_interval'] == 0:\n",
    "\n",
    "            # Print log\n",
    "            print(f'[Epoch {epoch}/{ args[\"epochs\"] }] [Batch {i}/{len(facades_dataloader)}] [D loss: {loss_D.item():.4f}] [G loss: {loss_G.item():.4f}, pixel: {loss_pixel.item():.4f}, adv: {loss_GAN.item():.4f}] ETA: {time_left}')\n",
    "            sample_images(facades_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c698e7be-4be0-4e9c-b863-8711e623cac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a460ba6-a395-4db8-8bda-d6ffbf84a798",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
