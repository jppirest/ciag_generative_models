{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a8020-fc6c-4b02-a13c-740d3839a130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "import datetime\n",
    "\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262b8d2-e264-4ba6-ab9b-263622057478",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'epochs': 200,\n",
    "    'img_size': 256,\n",
    "    'batch_size': 32,\n",
    "    'cuda': True if torch.cuda.is_available() else False,\n",
    "    'sample_interval': 20,\n",
    "    'checkpoint_interval': -1,\n",
    "    'dataset_name': 'cityscapes',\n",
    "\n",
    "    'lr': 0.0002,\n",
    "    'b1': 0.5,\n",
    "    'b2': 0.999,\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args['device'] = torch.device('cuda')\n",
    "else:\n",
    "    args['device'] = torch.device('cpu')\n",
    "\n",
    "print(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf5d5f1-d61c-4b2a-b62d-2983634b519c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root = '/pgeoprj/ciag2023/datasets/cityscapes/', transforms_= None, mode=\"train\"):\n",
    "        \n",
    "        \n",
    "        self.transform = transforms_\n",
    "\n",
    "        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        w, h = img.size\n",
    "        img_A = img.crop((0, 0, w / 2, h))\n",
    "        img_B = img.crop((w / 2, 0, w, h))\n",
    "\n",
    "        if np.random.random() < 0.5:\n",
    "            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
    "            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            img_A = self.transform(img_A)\n",
    "            img_B = self.transform(img_B)\n",
    "        \n",
    "        img_A = np.array(img_A)\n",
    "        img_B = np.array(img_B)\n",
    "        \n",
    "        img_A = (img_A.transpose(2, 0, 1) / 127.5) - 1.0\n",
    "        img_B = (img_B.transpose(2, 0, 1) / 127.5) - 1.0\n",
    "        \n",
    "        return img_B.astype(np.float32), img_A.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "transforms_ = transforms.Compose([\n",
    "    transforms.Resize( size=(args['img_size'], args['img_size']) ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298ccda-e6cf-48e5-a01f-c0d8ddecb9b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cityscapes = ImageDataset(transforms_ = transforms_)\n",
    "len(cityscapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a6e6a-e59f-412e-975d-cd8ff286c9f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cityscapes.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d37c55-fc60-43fe-af31-ff3cdcf72f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f0229-ab73-400c-9c1d-b814dc80ba86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cityscapes_dataloader = DataLoader(dataset = cityscapes,\n",
    "                                batch_size = args['batch_size'],\n",
    "                                shuffle = True,\n",
    "                                num_workers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d6c4c7-15b0-46fa-b9e9-a294054513fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models import GeneratorUNet, Discriminator\n",
    "generator = GeneratorUNet().to(args['device'])\n",
    "discriminator = Discriminator().to(args['device'])\n",
    "\n",
    "# Initialize weight\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8acb9-94f4-4cca-9275-374b5f7906c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_images(data_loader):\n",
    "    \n",
    "    imgs_real, label = next( iter(data_loader) )\n",
    "    imgs_real = imgs_real.to(args['device'])\n",
    "    label = label.to(args['device'])\n",
    "\n",
    "    imgs_fake = generator(label)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=min(imgs_real.size(0), 2), ncols=3, figsize=(12, 8))\n",
    "\n",
    "    for i in range( min(imgs_real.size(0), 2) ):\n",
    "\n",
    "        ax[i, 0].imshow(imgs_real.data[i].cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "        ax[i, 0].set_yticks([])\n",
    "        ax[i, 0].set_xticks([])\n",
    "        ax[i, 0].set_title('Real')\n",
    "\n",
    "        ax[i, 1].imshow(imgs_fake.data[i].cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "        ax[i, 1].set_yticks([])\n",
    "        ax[i, 1].set_xticks([])\n",
    "        ax[i, 1].set_title('Generated')\n",
    "\n",
    "        ax[i, 2].imshow(label.data[i].cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "        ax[i, 2].set_yticks([])\n",
    "        ax[i, 2].set_xticks([])\n",
    "        ax[i, 2].set_title('Label')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b49c5ae-ceaa-4558-af9f-63355c06b9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_images(cityscapes_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c800e-5c2e-41a7-b808-8f5c1d3869b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b8e31-663e-4c0f-a8ea-6fb70a71b572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
    "lambda_pixel = 100\n",
    "\n",
    "# Calculate output of image discriminator (PatchGAN)\n",
    "patch = (1, args['img_size'] // 2 ** 4, args['img_size'] // 2 ** 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ec699-7a83-41e5-9e80-d2ed1fc31a49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam( generator.parameters(),\n",
    "                               lr = args['lr'],\n",
    "                               betas = (args['b1'], args['b2']) )\n",
    "\n",
    "optimizer_D = torch.optim.Adam( discriminator.parameters(),\n",
    "                               lr = args['lr'],\n",
    "                               betas = (args['b1'], args['b2']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64f05b-0258-43ff-b599-4452f015ab2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#  Training ####################################################################\n",
    "################################################################################\n",
    "\n",
    "prev_time = time.time()\n",
    "\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "\n",
    "    for i, (real_img, label) in enumerate(cityscapes_dataloader):\n",
    "\n",
    "        # Transfer images and labels to GPU\n",
    "        real_img = real_img.to(args['device'])\n",
    "        label = label.to(args['device'])\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        y_true = torch.ones(size=(real_img.size(0), *patch), requires_grad=False).to(args['device'])\n",
    "        y_fake = torch.zeros(size=(real_img.size(0), *patch), requires_grad=False).to(args['device'])\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        # Clearing gradients for G optimizer.\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # GAN loss.\n",
    "        fake_img = generator(label)\n",
    "        pred_fake = discriminator(fake_img, label)\n",
    "        loss_GAN = criterion_GAN(pred_fake, y_true)\n",
    "\n",
    "        # Pixel-wise loss\n",
    "        loss_pixel = criterion_pixelwise(fake_img, real_img)\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = (loss_GAN + lambda_pixel * loss_pixel) / (1 + lambda_pixel)\n",
    "\n",
    "        # G backward and optimizer step.\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Clearing gradients for D optimizer.\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = discriminator(real_img, label)\n",
    "        loss_real = criterion_GAN(pred_real, y_true)\n",
    "\n",
    "        # Fake loss\n",
    "        pred_fake = discriminator(fake_img.detach(), label)\n",
    "        loss_fake = criterion_GAN(pred_fake, y_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D = 0.5 * (loss_real + loss_fake)\n",
    "\n",
    "        # D backward and optimizer step.\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(cityscapes_dataloader) + i\n",
    "        batches_left = args['epochs'] * len(cityscapes_dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # If at sample interval save image\n",
    "        if batches_done % args['sample_interval'] == 0:\n",
    "\n",
    "            # Print log\n",
    "            print(f'[Epoch {epoch}/{ args[\"epochs\"] }] [Batch {i}/{len(cityscapes_dataloader)}] [D loss: {loss_D.item():.4f}] [G loss: {loss_G.item():.4f}, pixel: {loss_pixel.item():.4f}, adv: {loss_GAN.item():.4f}] ETA: {time_left}')\n",
    "            sample_images(cityscapes_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab4138-bd01-455a-8b0e-0ba4a70e4dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscapes_validation = ImageDataset(transforms_ = transforms_, mode = 'val')\n",
    "cityscapes_validation_dataloader = DataLoader(dataset = cityscapes,\n",
    "                                batch_size = args['batch_size'],\n",
    "                                shuffle = True,\n",
    "                                num_workers = 1)\n",
    "\n",
    "def validate(dataloader):\n",
    "    \n",
    "    generator.eval()\n",
    "    with torch.no_grad()\n",
    "        sample_images(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165cb440-3961-43ab-b132-642927f6ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(cityscapes_validation_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
