{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6382d5e1-89f5-4dc3-b4cf-6b78f4ef2244",
   "metadata": {},
   "source": [
    "## M5 - pix2pix : Image-to-Image Translation\n",
    "\n",
    "For this notebook we will translate one image to another, as described in the [pix2pix article](https://arxiv.org/pdf/1611.07004.pdf). Using the `cityscapes` dataset, we will map from the original photo to its semantic map.\n",
    "Even though we are mapping from the original photo to its semantic map, we could do the other way around. That is the beauty of image2image translation.\n",
    "\n",
    "We will train a discriminator and a UNet generator to map from one image to another. We assume that this map exists, and through adversarial training we will try to obtain this mapping f:\n",
    "\n",
    "$$f: X \\to Y, \\,\\,\\, X,Y \\in \\mathbb{R}^{\\text{HxWxC}}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7a8020-fc6c-4b02-a13c-740d3839a130",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets\n",
    "import datetime\n",
    "\n",
    "from PIL import Image\n",
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6262b8d2-e264-4ba6-ab9b-263622057478",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'epochs': 200,\n",
    "    'img_size': 256,\n",
    "    'batch_size': 32,\n",
    "    'cuda': True if torch.cuda.is_available() else False,\n",
    "    'sample_interval': 20,\n",
    "    'checkpoint_interval': -1,\n",
    "    'dataset_name': 'cityscapes',\n",
    "\n",
    "    'lr': 0.0002,\n",
    "    'b1': 0.5,\n",
    "    'b2': 0.999,\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    args['device'] = torch.device('cuda')\n",
    "else:\n",
    "    args['device'] = torch.device('cpu')\n",
    "\n",
    "print(args['device'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652a59b7-00f4-45aa-9a73-eeffaf757853",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "Our image dataset is composed of multiple images. For the cityscapes dataset, the image and the \"label\" are present in the same file. To split these two images, we will crop the .jpg in half. For this we will use the `PIL` library to both load the image and crop it.\n",
    "\n",
    "It is worh mentioning that the loaded image is in format *(WxHxC)*, so the channels are in the last dimension of my array. As we know, `pytorch` expects the channel to be in the dimension. We will ensure that using the `transpose` function, to make the image *(CxWxH)*.\n",
    "\n",
    "Finally, we will scale the image from the range [0,255] to [-1,1], as it is more stable to train GANs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf5d5f1-d61c-4b2a-b62d-2983634b519c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root = '/pgeoprj/ciag2023/datasets/cityscapes/', transforms_= None, mode=\"train\"):\n",
    "        \n",
    "        \n",
    "        self.transform = transforms_\n",
    "\n",
    "        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        img = Image.open(self.files[index % len(self.files)])\n",
    "        w, h = img.size\n",
    "        img_A = img.crop((0, 0, w / 2, h))\n",
    "        img_B = img.crop((w / 2, 0, w, h))\n",
    "\n",
    "        if np.random.random() < 0.5:\n",
    "            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
    "            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
    "        \n",
    "        if self.transform:\n",
    "            img_A = self.transform(img_A)\n",
    "            img_B = self.transform(img_B)\n",
    "        \n",
    "        img_A = np.array(img_A)\n",
    "        img_B = np.array(img_B)\n",
    "        \n",
    "        img_A = (img_A.transpose(2, 0, 1) / 127.5) - 1.0\n",
    "        img_B = (img_B.transpose(2, 0, 1) / 127.5) - 1.0\n",
    "        \n",
    "        return img_B.astype(np.float32), img_A.astype(np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "transforms_ = transforms.Compose([\n",
    "    transforms.Resize( size=(args['img_size'], args['img_size']) ),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4298ccda-e6cf-48e5-a01f-c0d8ddecb9b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cityscapes = ImageDataset(transforms_ = transforms_)\n",
    "len(cityscapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d6f0229-ab73-400c-9c1d-b814dc80ba86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cityscapes_dataloader = DataLoader(dataset = cityscapes,\n",
    "                                batch_size = args['batch_size'],\n",
    "                                shuffle = True,\n",
    "                                num_workers = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ebfc00-e8a3-4aee-a379-7061284440c9",
   "metadata": {},
   "source": [
    "### Generator and Discriminator\n",
    "\n",
    "The discriminator is a pretty straightforward network: it receives two images (imgA, and imgB, which we want to obtain from imgA), which are concatenated along the channels and performs a downsample convolution using `nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)` to sequentially decrease the image in size and outputs a 1-d tensor, 0 or 1, stating if the passed image is fake or real. As we learn in [DCGANs](https://arxiv.org/pdf/1511.06434.pdf), we shall avoid Feed Forward Networks and ReLUs, so we will perform only convolutions and use the `LeakyReLU`.\n",
    "\n",
    "For the generator, we will use an [UNet](https://arxiv.org/pdf/1505.04597v1.pdf). We extensively studied the UNet, and it should be pretty clear how it was constructed and its advantages, due to its \"skip\" connection.\n",
    "\n",
    "Defining the `weight_init_normal` we will instantiate how the weights of our networks will be initialized.\n",
    "\n",
    "We will import it from the `models.py`, located in the previous folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d37c55-fc60-43fe-af31-ff3cdcf72f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def weights_init_normal(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find(\"Conv\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find(\"BatchNorm2d\") != -1:\n",
    "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        torch.nn.init.constant_(m.bias.data, 0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d6c4c7-15b0-46fa-b9e9-a294054513fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models import GeneratorUNet, Discriminator\n",
    "generator = GeneratorUNet().to(args['device'])\n",
    "discriminator = Discriminator().to(args['device'])\n",
    "\n",
    "# Initialize weight\n",
    "generator.apply(weights_init_normal)\n",
    "discriminator.apply(weights_init_normal);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45550124-fb73-4745-a52e-ff0b0fd47ff8",
   "metadata": {},
   "source": [
    "### Visualizing the generated image and its corresponding inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f8acb9-94f4-4cca-9275-374b5f7906c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sample_images(data_loader):\n",
    "    \n",
    "    imgs_real, label = next( iter(data_loader) )\n",
    "    imgs_real = imgs_real.to(args['device'])\n",
    "    label = label.to(args['device'])\n",
    "\n",
    "    imgs_fake = generator(label)\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=min(imgs_real.size(0), 2), ncols=3, figsize=(12, 8))\n",
    "\n",
    "    for i in range( min(imgs_real.size(0), 2) ):\n",
    "\n",
    "        ax[i, 0].imshow(imgs_real.data[i].cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "        ax[i, 0].set_yticks([])\n",
    "        ax[i, 0].set_xticks([])\n",
    "        ax[i, 0].set_title('Real')\n",
    "\n",
    "        ax[i, 1].imshow(imgs_fake.data[i].cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "        ax[i, 1].set_yticks([])\n",
    "        ax[i, 1].set_xticks([])\n",
    "        ax[i, 1].set_title('Generated')\n",
    "\n",
    "        ax[i, 2].imshow(label.data[i].cpu().numpy().transpose(1, 2, 0) * 0.5 + 0.5)\n",
    "        ax[i, 2].set_yticks([])\n",
    "        ax[i, 2].set_xticks([])\n",
    "        ax[i, 2].set_title('Label')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b49c5ae-ceaa-4558-af9f-63355c06b9ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sample_images(cityscapes_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5391861-5851-45bf-bffb-8d70260de0c5",
   "metadata": {},
   "source": [
    "### Defining the loss functions and the optimizers\n",
    "\n",
    "We will use the `nn.MSELoss`, due to the [Least Square GANs](https://arxiv.org/pdf/1611.04076.pdf). The original pix2pix defined a pixelwise loss, which we will calculate using the `nn.L1Loss`.\n",
    "\n",
    "For the optimizers, we will use `optim.Adam`, with the learning rate and weight decay defined on the `args` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10c800e-5c2e-41a7-b808-8f5c1d3869b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "criterion_GAN = torch.nn.MSELoss()\n",
    "criterion_pixelwise = torch.nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b8e31-663e-4c0f-a8ea-6fb70a71b572",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loss weight of L1 pixel-wise loss between translated image and real image\n",
    "lambda_pixel = 100\n",
    "\n",
    "# Calculate output of image discriminator (PatchGAN)\n",
    "patch = (1, args['img_size'] // 2 ** 4, args['img_size'] // 2 ** 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703ec699-7a83-41e5-9e80-d2ed1fc31a49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optimizers\n",
    "optimizer_G = torch.optim.Adam( generator.parameters(),\n",
    "                               lr = args['lr'],\n",
    "                               betas = (args['b1'], args['b2']) )\n",
    "\n",
    "optimizer_D = torch.optim.Adam( discriminator.parameters(),\n",
    "                               lr = args['lr'],\n",
    "                               betas = (args['b1'], args['b2']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eaf8b3-d0a9-4f2a-b8a3-af75137117d9",
   "metadata": {},
   "source": [
    "## Training procedure\n",
    "\n",
    "\n",
    "The overall workflow goes as follows:\n",
    "\n",
    "    1. Generate fake image using the generator\n",
    "        2. Get the discriminator prediction for the fake image\n",
    "        3. Calculate the generator loss using the MSE and the pixelwise loss.\n",
    "    \n",
    "    4. Get the discriminator prediction for the real image \n",
    "        5. Calculate the discriminator loss using MSE for the real image.\n",
    "    \n",
    "    6. Get the discriminator prediction for the fake image (with torch.no_grad()).\n",
    "        7. Calculate the discriminator loss using MSE for the fake image.\n",
    "\n",
    "### Losses\n",
    "\n",
    "> Generator \n",
    "\n",
    "The `loss_GAN` is responsible for applying the MSE between the output of the discriminator and the `y_true` which is always 1. Therefore, this `loss_GAN`, which will be used into the `loss_G` (Generator loss), is basically tricking the discriminator to predict 1, even though the passed image to the discriminator is fake.\n",
    "\n",
    "The pixelwise loss, `loss_pixel` calculates the difference between the predicted image and the real image, that is between what my generator generated, and what it would ideally generate.\n",
    "\n",
    "Finally, we sum these two losses balancing using the `lambda_pixel`, due to PatchGAN.\n",
    "\n",
    "> Discriminator\n",
    "\n",
    "`pred_real` calculates the discriminator output for the real image. `loss_real` calculates the loss between this ouput and `y_true`, which is always 1. So we are tricking the discriminator to predict 1 to the real image.\n",
    "\n",
    "`pred_fake` calculates the discriminator output for the fake image. `loss_fake` calculates the loss between this ouput and `y_fake`, which is always 0. So we are tricking the discriminator to predict 0 to the fake image.\n",
    "\n",
    "Finally, we take the mean of the `loss_fake` and `loss_real`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d64f05b-0258-43ff-b599-4452f015ab2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################\n",
    "#  Training ####################################################################\n",
    "################################################################################\n",
    "\n",
    "prev_time = time.time()\n",
    "\n",
    "for epoch in range(1, args['epochs'] + 1):\n",
    "\n",
    "    for i, (real_img, label) in enumerate(cityscapes_dataloader):\n",
    "\n",
    "        # Transfer images and labels to GPU\n",
    "        real_img = real_img.to(args['device'])\n",
    "        label = label.to(args['device'])\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        y_true = torch.ones(size=(real_img.size(0), *patch), requires_grad=False).to(args['device'])\n",
    "        y_fake = torch.zeros(size=(real_img.size(0), *patch), requires_grad=False).to(args['device'])\n",
    "\n",
    "        # ------------------\n",
    "        #  Train Generators\n",
    "        # ------------------\n",
    "\n",
    "        # Clearing gradients for G optimizer.\n",
    "        optimizer_G.zero_grad()\n",
    "\n",
    "        # GAN loss.\n",
    "        fake_img = generator(label)\n",
    "        pred_fake = discriminator(fake_img, label)\n",
    "        loss_GAN = criterion_GAN(pred_fake, y_true)\n",
    "\n",
    "        # Pixel-wise loss\n",
    "        loss_pixel = criterion_pixelwise(fake_img, real_img)\n",
    "\n",
    "        # Total loss\n",
    "        loss_G = (loss_GAN + lambda_pixel * loss_pixel) / (1 + lambda_pixel)\n",
    "\n",
    "        # G backward and optimizer step.\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # ---------------------\n",
    "        #  Train Discriminator\n",
    "        # ---------------------\n",
    "\n",
    "        # Clearing gradients for D optimizer.\n",
    "        optimizer_D.zero_grad()\n",
    "\n",
    "        # Real loss\n",
    "        pred_real = discriminator(real_img, label)\n",
    "        loss_real = criterion_GAN(pred_real, y_true)\n",
    "\n",
    "        # Fake loss\n",
    "        pred_fake = discriminator(fake_img.detach(), label)\n",
    "        loss_fake = criterion_GAN(pred_fake, y_fake)\n",
    "\n",
    "        # Total loss\n",
    "        loss_D = 0.5 * (loss_real + loss_fake)\n",
    "\n",
    "        # D backward and optimizer step.\n",
    "        loss_D.backward()\n",
    "        optimizer_D.step()\n",
    "\n",
    "        # --------------\n",
    "        #  Log Progress\n",
    "        # --------------\n",
    "\n",
    "        # Determine approximate time left\n",
    "        batches_done = epoch * len(cityscapes_dataloader) + i\n",
    "        batches_left = args['epochs'] * len(cityscapes_dataloader) - batches_done\n",
    "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "        prev_time = time.time()\n",
    "\n",
    "        # If at sample interval save image\n",
    "        if batches_done % args['sample_interval'] == 0:\n",
    "\n",
    "            # Print log\n",
    "            print(f'[Epoch {epoch}/{ args[\"epochs\"] }] [Batch {i}/{len(cityscapes_dataloader)}] [D loss: {loss_D.item():.4f}] [G loss: {loss_G.item():.4f}, pixel: {loss_pixel.item():.4f}, adv: {loss_GAN.item():.4f}] ETA: {time_left}')\n",
    "            sample_images(cityscapes_dataloader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1f049b-3673-4791-a4d4-aff7290f533a",
   "metadata": {},
   "source": [
    "### Validating the generator network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ab4138-bd01-455a-8b0e-0ba4a70e4dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cityscapes_validation = ImageDataset(transforms_ = transforms_, mode = 'val')\n",
    "cityscapes_validation_dataloader = DataLoader(dataset = cityscapes,\n",
    "                                batch_size = args['batch_size'],\n",
    "                                shuffle = True,\n",
    "                                num_workers = 1)\n",
    "\n",
    "def validate(dataloader):\n",
    "    \n",
    "    generator.eval()\n",
    "    with torch.no_grad()\n",
    "        sample_images(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165cb440-3961-43ab-b132-642927f6ca65",
   "metadata": {},
   "outputs": [],
   "source": [
    "validate(cityscapes_validation_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
